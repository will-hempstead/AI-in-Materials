{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2123c837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as Fy\n",
    "from torch.utils import data\n",
    "import math\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import sys\n",
    "from os.path import exists\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.rdmolops import GetAdjacencyMatrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import from_smiles\n",
    "from torch.utils.data import random_split\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn.models import AttentiveFP\n",
    "\n",
    "import build_graphs as bg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18305704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                smiles    gap\n",
      "0    COC(=O)/C(=C/c1cc(C)c(c2ccc(c3sc(c4cc5c(s4)c(O...  0.235\n",
      "1                  Cc1csc(c2ccc(c3cc(C)cs3)c3nsnc23)c1  0.262\n",
      "2    COC(=O)/C(=C/c1cc(C)c(c2cc(C)c(c3ccc(c4sc(c5sc...  0.234\n",
      "3    C[Si]1(C)c2ccsc2c2sc(c3nc4sc(c5cc6c(s5)c5sc(c7...  0.245\n",
      "4    COc1c2ccsc2c(OC)c2cc(c3sc(c4scc5c4[C@@H]4C=C[C...  0.300\n",
      "..                                                 ...    ...\n",
      "311  Cc1c2ccsc2c(C)c2cc(c3ccc(c4cnc(c5cccs5)c5nsnc4...  0.236\n",
      "312       CC(=O)c1cc2c(csc2c2cc3c(s2)c(C)c2ccsc2c3C)s1  0.276\n",
      "313  Cc1cc(c2ccc(N(c3ccccc3)c3ccccc3)cc2)sc1c1cnc(c...  0.258\n",
      "314  Cc1ccc(C2(c3ccc(C)cc3)c3ccsc3c3cc4c(cc23)c2sc(...  0.255\n",
      "315  Cc1cc(c2cc3c4nsnc4c(c4cc(C)c(c5cccs5)s4)cc3c3n...  0.231\n",
      "\n",
      "[316 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_pickle('mat_bandgap_morgan.pkl')\n",
    "df = df.drop(['Morgan','confnum','homo','lumo'],axis=1)\n",
    "mols = [Chem.MolFromSmiles(x) for x in df['smiles']]\n",
    "print(df)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "bandgaps = df['gap'].values.reshape(-1, 1)\n",
    "bandgaps_scaled = scaler.fit_transform(bandgaps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70f8f316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c1csc(c2ccsc2c2cccs2)c1\n"
     ]
    }
   ],
   "source": [
    "# example for one molecule:\n",
    "\n",
    "smile = df['smiles'][35]\n",
    "print(smile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3232641f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[23, 16], edge_index=[2, 50], edge_attr=[50, 3], smiles='c1csc(c2ccsc2c2cccs2)c1')\n"
     ]
    }
   ],
   "source": [
    "# this generates a torch Data object for the molecule\n",
    "\n",
    "g = bg.from_smiles(smile, with_hydrogen=True, kekulize=True, use_3d=True)\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1041783e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6,  0,  3,  5,  0,  0,  3,  1,  1,  1,  4,  0,  4, 12,  0,  5],\n",
       "        [ 6,  0,  3,  5,  0,  0,  3,  1,  1,  1,  4,  0,  4, 12,  0,  5],\n",
       "        [16,  0,  2,  5,  0,  0,  3,  1,  1,  1,  2,  0,  2, 32,  0,  5],\n",
       "        [ 6,  0,  3,  5,  0,  0,  3,  1,  1,  1,  4,  0,  4, 12,  0,  5],\n",
       "        [ 6,  0,  3,  5,  0,  0,  3,  1,  1,  1,  4,  0,  4, 12,  0,  5],\n",
       "        [ 6,  0,  3,  5,  0,  0,  3,  1,  1,  1,  4,  0,  4, 12,  0,  5],\n",
       "        [ 6,  0,  3,  5,  0,  0,  3,  1,  1,  1,  4,  0,  4, 12,  0,  5],\n",
       "        [16,  0,  2,  5,  0,  0,  3,  1,  1,  1,  2,  0,  2, 32,  0,  5],\n",
       "        [ 6,  0,  3,  5,  0,  0,  3,  1,  1,  1,  4,  0,  4, 12,  0,  5],\n",
       "        [ 6,  0,  3,  5,  0,  0,  3,  1,  1,  1,  4,  0,  4, 12,  0,  5],\n",
       "        [ 6,  0,  3,  5,  0,  0,  3,  1,  1,  1,  4,  0,  4, 12,  0,  5],\n",
       "        [ 6,  0,  3,  5,  0,  0,  3,  1,  1,  1,  4,  0,  4, 12,  0,  5],\n",
       "        [ 6,  0,  3,  5,  0,  0,  3,  1,  1,  1,  4,  0,  4, 12,  0,  5],\n",
       "        [16,  0,  2,  5,  0,  0,  3,  1,  1,  1,  2,  0,  2, 32,  0,  5],\n",
       "        [ 6,  0,  3,  5,  0,  0,  3,  1,  1,  1,  4,  0,  4, 12,  0,  5],\n",
       "        [ 1,  0,  1,  5,  0,  0,  0,  0,  0,  0,  1,  0,  1,  1,  0,  0],\n",
       "        [ 1,  0,  1,  5,  0,  0,  0,  0,  0,  0,  1,  0,  1,  1,  0,  0],\n",
       "        [ 1,  0,  1,  5,  0,  0,  0,  0,  0,  0,  1,  0,  1,  1,  0,  0],\n",
       "        [ 1,  0,  1,  5,  0,  0,  0,  0,  0,  0,  1,  0,  1,  1,  0,  0],\n",
       "        [ 1,  0,  1,  5,  0,  0,  0,  0,  0,  0,  1,  0,  1,  1,  0,  0],\n",
       "        [ 1,  0,  1,  5,  0,  0,  0,  0,  0,  0,  1,  0,  1,  1,  0,  0],\n",
       "        [ 1,  0,  1,  5,  0,  0,  0,  0,  0,  0,  1,  0,  1,  1,  0,  0],\n",
       "        [ 1,  0,  1,  5,  0,  0,  0,  0,  0,  0,  1,  0,  1,  1,  0,  0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this prints the node feature matrix - so all the atoms in the molecule and some metrics for them\n",
    "g.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2633753d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this prints the number of nodes:\n",
    "\n",
    "g.num_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96067ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.num_node_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9938d203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  1,  1,  1,  2,  2,  3,  3,  3,  4,  4,  4,  5,  5,  5,  6,\n",
       "          6,  6,  7,  7,  8,  8,  8,  9,  9,  9, 10, 10, 10, 11, 11, 11, 12, 12,\n",
       "         12, 13, 13, 14, 14, 14, 15, 16, 17, 18, 19, 20, 21, 22],\n",
       "        [ 1, 14, 15,  0,  2, 16,  1,  3,  2,  4, 14,  3,  5,  8,  4,  6, 17,  5,\n",
       "          7, 18,  6,  8,  4,  7,  9,  8, 10, 13,  9, 11, 19, 10, 12, 20, 11, 13,\n",
       "         21,  9, 12,  0,  3, 22,  0,  1,  5,  6, 10, 11, 12, 14]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b960cb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 0, 1],\n",
       "        [1, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [2, 0, 1],\n",
       "        [1, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 1],\n",
       "        [1, 0, 1],\n",
       "        [1, 0, 1],\n",
       "        [1, 0, 1],\n",
       "        [2, 0, 1],\n",
       "        [1, 0, 1],\n",
       "        [1, 0, 1],\n",
       "        [2, 0, 1],\n",
       "        [1, 0, 1],\n",
       "        [2, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [2, 0, 1],\n",
       "        [1, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 1],\n",
       "        [1, 0, 1],\n",
       "        [2, 0, 1],\n",
       "        [1, 0, 1],\n",
       "        [1, 0, 1],\n",
       "        [1, 0, 1],\n",
       "        [2, 0, 1],\n",
       "        [1, 0, 1],\n",
       "        [2, 0, 1],\n",
       "        [1, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 1],\n",
       "        [2, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [2, 0, 1],\n",
       "        [1, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 1],\n",
       "        [1, 0, 1],\n",
       "        [1, 0, 1],\n",
       "        [2, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c727116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this generates data objects for all the molecules in the dataset\n",
    "\n",
    "graph_list = []\n",
    "\n",
    "for i, smile in enumerate(df['smiles']):\n",
    "    g = bg.from_smiles(smile)\n",
    "    g.x = g.x.float()\n",
    "    y = torch.tensor(bandgaps_scaled[i],dtype=torch.float).view(1,-1)\n",
    "    g.y = y\n",
    "    graph_list.append(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1777eb35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch_geometric.data.data.Data"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(graph_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cba0b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[21, 16], edge_index=[2, 48], edge_attr=[48, 3], smiles='Cc1csc(c2ccc(c3cc(C)cs3)c3nsnc23)c1', y=[1, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ee9cf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "\n",
    "train_ratio = 0.80\n",
    "dataset_size = len(graph_list)\n",
    "train_size = int(train_ratio*dataset_size)\n",
    "test_size = dataset_size-train_size\n",
    "\n",
    "generator1 = torch.Generator().manual_seed(42)\n",
    "train_dataset, test_dataset = random_split(graph_list,[train_size,test_size], generator=generator1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0aa95261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed3800ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60417293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[66, 16], edge_index=[2, 148], edge_attr=[148, 3], smiles='COC(=O)/C(=C/c1cc(C)c(c2ccc(c3sc(c4cc5c(s4)c(OC)c4cc(c6cc(C)c(c7ccc(c8sc(/C=C(\\C#N)/C(=O)OC)cc8C)s7)s6)sc4c5OC)cc3C)s2)s1)/C#N', y=[1, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8c8df4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = AttentiveFP(in_channels=16,hidden_channels=64,out_channels=1,edge_dim=3,num_layers=2,num_timesteps=2,dropout=0.4)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001,weight_decay=1e-4)\n",
    "\n",
    "loss_function = nn.MSELoss()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c5722fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttentiveFP(in_channels=11, hidden_channels=64, out_channels=1, edge_dim=3, num_layers=2, num_timesteps=2)\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f11c157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss: 27.9217 Train R²: -26.6300 Test Loss: 2.6284 Test R²: -1.8834\n",
      "1 Train Loss: 5.1445 Train R²: -4.0784 Test Loss: 1.1823 Test R²: -0.2971\n",
      "2 Train Loss: 1.3380 Train R²: -0.3226 Test Loss: 0.6895 Test R²: 0.2436\n",
      "3 Train Loss: 1.1303 Train R²: -0.1056 Test Loss: 0.6387 Test R²: 0.2994\n",
      "4 Train Loss: 0.9978 Train R²: 0.0129 Test Loss: 0.6220 Test R²: 0.3177\n",
      "5 Train Loss: 0.8521 Train R²: 0.1574 Test Loss: 0.7477 Test R²: 0.1797\n",
      "6 Train Loss: 0.8738 Train R²: 0.1388 Test Loss: 1.6490 Test R²: -0.8090\n",
      "7 Train Loss: 0.8334 Train R²: 0.1817 Test Loss: 0.7676 Test R²: 0.1579\n",
      "8 Train Loss: 0.8403 Train R²: 0.1724 Test Loss: 0.6705 Test R²: 0.2644\n",
      "9 Train Loss: 0.7780 Train R²: 0.2393 Test Loss: 0.6770 Test R²: 0.2573\n",
      "10 Train Loss: 0.6508 Train R²: 0.3616 Test Loss: 0.8165 Test R²: 0.1043\n",
      "11 Train Loss: 0.7319 Train R²: 0.2866 Test Loss: 0.5393 Test R²: 0.4084\n",
      "12 Train Loss: 0.7553 Train R²: 0.2714 Test Loss: 0.5487 Test R²: 0.3980\n",
      "13 Train Loss: 0.7637 Train R²: 0.2478 Test Loss: 0.5283 Test R²: 0.4204\n",
      "14 Train Loss: 0.6507 Train R²: 0.3682 Test Loss: 0.5379 Test R²: 0.4099\n",
      "15 Train Loss: 0.7054 Train R²: 0.3099 Test Loss: 0.6571 Test R²: 0.2791\n",
      "16 Train Loss: 0.6949 Train R²: 0.3170 Test Loss: 0.4843 Test R²: 0.4687\n",
      "17 Train Loss: 0.6796 Train R²: 0.3409 Test Loss: 0.4856 Test R²: 0.4673\n",
      "18 Train Loss: 0.7403 Train R²: 0.2727 Test Loss: 0.4657 Test R²: 0.4891\n",
      "19 Train Loss: 0.6476 Train R²: 0.3695 Test Loss: 0.4846 Test R²: 0.4684\n",
      "20 Train Loss: 0.6724 Train R²: 0.3384 Test Loss: 0.5776 Test R²: 0.3663\n",
      "21 Train Loss: 0.6463 Train R²: 0.3665 Test Loss: 0.4668 Test R²: 0.4880\n",
      "22 Train Loss: 0.6875 Train R²: 0.3243 Test Loss: 0.4802 Test R²: 0.4731\n",
      "23 Train Loss: 0.6863 Train R²: 0.3313 Test Loss: 0.4532 Test R²: 0.5028\n",
      "24 Train Loss: 0.6480 Train R²: 0.3663 Test Loss: 0.4715 Test R²: 0.4828\n",
      "25 Train Loss: 0.6142 Train R²: 0.3985 Test Loss: 0.4628 Test R²: 0.4923\n",
      "26 Train Loss: 0.6466 Train R²: 0.3668 Test Loss: 0.4571 Test R²: 0.4985\n",
      "27 Train Loss: 0.6006 Train R²: 0.4123 Test Loss: 0.4705 Test R²: 0.4838\n",
      "28 Train Loss: 0.5930 Train R²: 0.4212 Test Loss: 0.5001 Test R²: 0.4514\n",
      "29 Train Loss: 0.6066 Train R²: 0.4119 Test Loss: 0.5097 Test R²: 0.4408\n",
      "30 Train Loss: 0.5877 Train R²: 0.4222 Test Loss: 0.4548 Test R²: 0.5011\n",
      "31 Train Loss: 0.6549 Train R²: 0.3606 Test Loss: 0.5123 Test R²: 0.4380\n",
      "32 Train Loss: 0.7346 Train R²: 0.2768 Test Loss: 0.5030 Test R²: 0.4482\n",
      "33 Train Loss: 0.5822 Train R²: 0.4323 Test Loss: 0.4581 Test R²: 0.4975\n",
      "34 Train Loss: 0.5500 Train R²: 0.4598 Test Loss: 0.4248 Test R²: 0.5339\n",
      "35 Train Loss: 0.6184 Train R²: 0.3955 Test Loss: 0.4267 Test R²: 0.5319\n",
      "36 Train Loss: 0.6467 Train R²: 0.3639 Test Loss: 0.4680 Test R²: 0.4866\n",
      "37 Train Loss: 0.5172 Train R²: 0.4952 Test Loss: 0.4121 Test R²: 0.5479\n",
      "38 Train Loss: 0.5731 Train R²: 0.4427 Test Loss: 0.3964 Test R²: 0.5652\n",
      "39 Train Loss: 0.5812 Train R²: 0.4279 Test Loss: 0.4092 Test R²: 0.5511\n",
      "40 Train Loss: 0.5861 Train R²: 0.4244 Test Loss: 0.4418 Test R²: 0.5154\n",
      "41 Train Loss: 0.6050 Train R²: 0.4100 Test Loss: 0.4403 Test R²: 0.5169\n",
      "42 Train Loss: 0.5788 Train R²: 0.4351 Test Loss: 0.4197 Test R²: 0.5395\n",
      "43 Train Loss: 0.6258 Train R²: 0.3908 Test Loss: 0.4261 Test R²: 0.5325\n",
      "44 Train Loss: 0.5656 Train R²: 0.4460 Test Loss: 0.4128 Test R²: 0.5471\n",
      "45 Train Loss: 0.5843 Train R²: 0.4331 Test Loss: 0.4253 Test R²: 0.5334\n",
      "46 Train Loss: 0.5200 Train R²: 0.4902 Test Loss: 0.3989 Test R²: 0.5623\n",
      "47 Train Loss: 0.5403 Train R²: 0.4681 Test Loss: 0.4729 Test R²: 0.4812\n",
      "48 Train Loss: 0.5264 Train R²: 0.4819 Test Loss: 0.4313 Test R²: 0.5269\n",
      "49 Train Loss: 0.4643 Train R²: 0.5457 Test Loss: 0.4912 Test R²: 0.4611\n",
      "50 Train Loss: 0.5667 Train R²: 0.4426 Test Loss: 0.5563 Test R²: 0.3897\n",
      "51 Train Loss: 0.5056 Train R²: 0.5082 Test Loss: 0.4215 Test R²: 0.5376\n",
      "52 Train Loss: 0.5520 Train R²: 0.4655 Test Loss: 0.4234 Test R²: 0.5355\n",
      "53 Train Loss: 0.5673 Train R²: 0.4505 Test Loss: 0.4199 Test R²: 0.5394\n",
      "54 Train Loss: 0.5594 Train R²: 0.4526 Test Loss: 0.4488 Test R²: 0.5076\n",
      "55 Train Loss: 0.5014 Train R²: 0.5057 Test Loss: 0.4646 Test R²: 0.4903\n",
      "56 Train Loss: 0.5413 Train R²: 0.4753 Test Loss: 0.5472 Test R²: 0.3997\n",
      "57 Train Loss: 0.5185 Train R²: 0.4890 Test Loss: 0.4583 Test R²: 0.4972\n",
      "58 Train Loss: 0.6675 Train R²: 0.3540 Test Loss: 0.4968 Test R²: 0.4550\n",
      "59 Train Loss: 0.6424 Train R²: 0.3744 Test Loss: 0.4379 Test R²: 0.5196\n",
      "60 Train Loss: 0.6248 Train R²: 0.3833 Test Loss: 0.4581 Test R²: 0.4974\n",
      "61 Train Loss: 0.5369 Train R²: 0.4757 Test Loss: 1.1545 Test R²: -0.2666\n",
      "62 Train Loss: 0.6473 Train R²: 0.3640 Test Loss: 0.5196 Test R²: 0.4300\n",
      "63 Train Loss: 0.5858 Train R²: 0.4273 Test Loss: 0.5064 Test R²: 0.4444\n",
      "64 Train Loss: 0.6077 Train R²: 0.4021 Test Loss: 0.4207 Test R²: 0.5384\n",
      "65 Train Loss: 0.6220 Train R²: 0.3916 Test Loss: 0.4216 Test R²: 0.5375\n",
      "66 Train Loss: 0.5400 Train R²: 0.4680 Test Loss: 0.4624 Test R²: 0.4927\n",
      "67 Train Loss: 0.6268 Train R²: 0.3917 Test Loss: 0.4578 Test R²: 0.4978\n",
      "68 Train Loss: 0.5453 Train R²: 0.4666 Test Loss: 0.3947 Test R²: 0.5670\n",
      "69 Train Loss: 0.5287 Train R²: 0.4864 Test Loss: 0.4526 Test R²: 0.5035\n",
      "70 Train Loss: 0.5410 Train R²: 0.4702 Test Loss: 0.4045 Test R²: 0.5562\n",
      "71 Train Loss: 0.5144 Train R²: 0.4951 Test Loss: 0.4414 Test R²: 0.5158\n",
      "72 Train Loss: 0.5839 Train R²: 0.4318 Test Loss: 0.4903 Test R²: 0.4621\n",
      "73 Train Loss: 0.5071 Train R²: 0.5062 Test Loss: 0.3889 Test R²: 0.5734\n",
      "74 Train Loss: 0.6132 Train R²: 0.4002 Test Loss: 0.4430 Test R²: 0.5140\n",
      "75 Train Loss: 0.5552 Train R²: 0.4523 Test Loss: 0.3839 Test R²: 0.5788\n",
      "76 Train Loss: 0.5211 Train R²: 0.4913 Test Loss: 0.4185 Test R²: 0.5409\n",
      "77 Train Loss: 0.4888 Train R²: 0.5214 Test Loss: 0.3756 Test R²: 0.5880\n",
      "78 Train Loss: 0.5215 Train R²: 0.4907 Test Loss: 0.4748 Test R²: 0.4791\n",
      "79 Train Loss: 0.5327 Train R²: 0.4760 Test Loss: 0.3915 Test R²: 0.5705\n",
      "80 Train Loss: 0.5331 Train R²: 0.4786 Test Loss: 0.3988 Test R²: 0.5625\n",
      "81 Train Loss: 0.5181 Train R²: 0.4934 Test Loss: 0.3929 Test R²: 0.5690\n",
      "82 Train Loss: 0.4981 Train R²: 0.5115 Test Loss: 0.3941 Test R²: 0.5677\n",
      "83 Train Loss: 0.5475 Train R²: 0.4596 Test Loss: 0.3988 Test R²: 0.5625\n",
      "84 Train Loss: 0.4704 Train R²: 0.5416 Test Loss: 0.3854 Test R²: 0.5772\n",
      "85 Train Loss: 0.5114 Train R²: 0.4970 Test Loss: 0.3708 Test R²: 0.5933\n",
      "86 Train Loss: 0.5241 Train R²: 0.4871 Test Loss: 0.3906 Test R²: 0.5715\n",
      "87 Train Loss: 0.4971 Train R²: 0.5129 Test Loss: 0.3883 Test R²: 0.5740\n",
      "88 Train Loss: 0.4825 Train R²: 0.5286 Test Loss: 0.3921 Test R²: 0.5699\n",
      "89 Train Loss: 0.5135 Train R²: 0.4952 Test Loss: 0.4854 Test R²: 0.4675\n",
      "90 Train Loss: 0.4607 Train R²: 0.5517 Test Loss: 0.3812 Test R²: 0.5818\n",
      "91 Train Loss: 0.5362 Train R²: 0.4780 Test Loss: 0.3992 Test R²: 0.5621\n",
      "92 Train Loss: 0.4297 Train R²: 0.5772 Test Loss: 0.5722 Test R²: 0.3723\n",
      "93 Train Loss: 0.4466 Train R²: 0.5600 Test Loss: 0.6568 Test R²: 0.2795\n",
      "94 Train Loss: 0.4799 Train R²: 0.5284 Test Loss: 0.4536 Test R²: 0.5024\n",
      "95 Train Loss: 0.4162 Train R²: 0.5945 Test Loss: 0.6347 Test R²: 0.3037\n",
      "96 Train Loss: 0.5257 Train R²: 0.4848 Test Loss: 0.4288 Test R²: 0.5296\n",
      "97 Train Loss: 0.4939 Train R²: 0.5138 Test Loss: 0.6995 Test R²: 0.2327\n",
      "98 Train Loss: 0.4662 Train R²: 0.5443 Test Loss: 0.3967 Test R²: 0.5648\n",
      "99 Train Loss: 0.4589 Train R²: 0.5517 Test Loss: 0.4039 Test R²: 0.5569\n",
      "100 Train Loss: 0.5506 Train R²: 0.4605 Test Loss: 0.3894 Test R²: 0.5728\n",
      "101 Train Loss: 0.5112 Train R²: 0.4992 Test Loss: 0.4434 Test R²: 0.5135\n",
      "102 Train Loss: 0.5101 Train R²: 0.5045 Test Loss: 0.7097 Test R²: 0.2214\n",
      "103 Train Loss: 0.4284 Train R²: 0.5772 Test Loss: 0.5525 Test R²: 0.3939\n",
      "104 Train Loss: 0.4558 Train R²: 0.5509 Test Loss: 0.4237 Test R²: 0.5351\n",
      "105 Train Loss: 0.4352 Train R²: 0.5727 Test Loss: 0.4381 Test R²: 0.5194\n",
      "106 Train Loss: 0.4552 Train R²: 0.5518 Test Loss: 0.8915 Test R²: 0.0220\n",
      "107 Train Loss: 0.4623 Train R²: 0.5494 Test Loss: 0.4172 Test R²: 0.5423\n",
      "108 Train Loss: 0.4606 Train R²: 0.5537 Test Loss: 0.4145 Test R²: 0.5453\n",
      "109 Train Loss: 0.4498 Train R²: 0.5639 Test Loss: 0.4247 Test R²: 0.5340\n",
      "110 Train Loss: 0.4304 Train R²: 0.5781 Test Loss: 0.4621 Test R²: 0.4930\n",
      "111 Train Loss: 0.4345 Train R²: 0.5718 Test Loss: 0.4342 Test R²: 0.5237\n",
      "112 Train Loss: 0.5650 Train R²: 0.4483 Test Loss: 0.4347 Test R²: 0.5231\n",
      "113 Train Loss: 0.4564 Train R²: 0.5521 Test Loss: 0.4043 Test R²: 0.5565\n",
      "114 Train Loss: 0.4368 Train R²: 0.5732 Test Loss: 0.3751 Test R²: 0.5885\n",
      "115 Train Loss: 0.4391 Train R²: 0.5729 Test Loss: 0.4262 Test R²: 0.5324\n",
      "116 Train Loss: 0.4108 Train R²: 0.5951 Test Loss: 0.3723 Test R²: 0.5916\n",
      "117 Train Loss: 0.4720 Train R²: 0.5361 Test Loss: 0.3876 Test R²: 0.5748\n",
      "118 Train Loss: 0.4307 Train R²: 0.5805 Test Loss: 0.3711 Test R²: 0.5928\n",
      "119 Train Loss: 0.4175 Train R²: 0.5954 Test Loss: 0.3556 Test R²: 0.6099\n",
      "120 Train Loss: 0.4561 Train R²: 0.5535 Test Loss: 0.4389 Test R²: 0.5186\n",
      "121 Train Loss: 0.4772 Train R²: 0.5342 Test Loss: 0.3597 Test R²: 0.6054\n",
      "122 Train Loss: 0.4328 Train R²: 0.5796 Test Loss: 0.4065 Test R²: 0.5541\n",
      "123 Train Loss: 0.4892 Train R²: 0.5270 Test Loss: 0.3531 Test R²: 0.6126\n",
      "124 Train Loss: 0.4004 Train R²: 0.6075 Test Loss: 0.3199 Test R²: 0.6491\n",
      "125 Train Loss: 0.5152 Train R²: 0.4916 Test Loss: 0.5184 Test R²: 0.4313\n",
      "126 Train Loss: 0.5200 Train R²: 0.4886 Test Loss: 0.3635 Test R²: 0.6012\n",
      "127 Train Loss: 0.4297 Train R²: 0.5769 Test Loss: 0.3430 Test R²: 0.6238\n",
      "128 Train Loss: 0.4242 Train R²: 0.5879 Test Loss: 0.4878 Test R²: 0.4649\n",
      "129 Train Loss: 0.4332 Train R²: 0.5744 Test Loss: 0.3681 Test R²: 0.5962\n",
      "130 Train Loss: 0.4358 Train R²: 0.5730 Test Loss: 0.4844 Test R²: 0.4686\n",
      "131 Train Loss: 0.4518 Train R²: 0.5614 Test Loss: 0.3384 Test R²: 0.6288\n",
      "132 Train Loss: 0.4602 Train R²: 0.5498 Test Loss: 0.4290 Test R²: 0.5293\n",
      "133 Train Loss: 0.3583 Train R²: 0.6498 Test Loss: 0.3375 Test R²: 0.6297\n",
      "134 Train Loss: 0.4670 Train R²: 0.5425 Test Loss: 0.3639 Test R²: 0.6007\n",
      "135 Train Loss: 0.4629 Train R²: 0.5450 Test Loss: 0.3880 Test R²: 0.5744\n",
      "136 Train Loss: 0.4686 Train R²: 0.5426 Test Loss: 0.3141 Test R²: 0.6554\n",
      "137 Train Loss: 0.4609 Train R²: 0.5474 Test Loss: 0.3495 Test R²: 0.6166\n",
      "138 Train Loss: 0.3612 Train R²: 0.6482 Test Loss: 0.4631 Test R²: 0.4919\n",
      "139 Train Loss: 0.3671 Train R²: 0.6387 Test Loss: 0.3528 Test R²: 0.6130\n",
      "140 Train Loss: 0.4376 Train R²: 0.5737 Test Loss: 0.3839 Test R²: 0.5788\n",
      "141 Train Loss: 0.4367 Train R²: 0.5721 Test Loss: 0.3503 Test R²: 0.6157\n",
      "142 Train Loss: 0.4165 Train R²: 0.5936 Test Loss: 0.3502 Test R²: 0.6158\n",
      "143 Train Loss: 0.4088 Train R²: 0.5984 Test Loss: 0.3891 Test R²: 0.5732\n",
      "144 Train Loss: 0.4415 Train R²: 0.5672 Test Loss: 0.3687 Test R²: 0.5956\n",
      "145 Train Loss: 0.3895 Train R²: 0.6193 Test Loss: 0.3840 Test R²: 0.5788\n",
      "146 Train Loss: 0.4162 Train R²: 0.5929 Test Loss: 0.3389 Test R²: 0.6282\n",
      "147 Train Loss: 0.4377 Train R²: 0.5712 Test Loss: 0.3136 Test R²: 0.6560\n",
      "148 Train Loss: 0.4752 Train R²: 0.5315 Test Loss: 0.4182 Test R²: 0.5413\n",
      "149 Train Loss: 0.3740 Train R²: 0.6322 Test Loss: 0.3219 Test R²: 0.6468\n",
      "150 Train Loss: 0.3866 Train R²: 0.6208 Test Loss: 0.3261 Test R²: 0.6423\n",
      "151 Train Loss: 0.4358 Train R²: 0.5817 Test Loss: 0.3318 Test R²: 0.6360\n",
      "152 Train Loss: 0.3895 Train R²: 0.6196 Test Loss: 0.3120 Test R²: 0.6578\n",
      "153 Train Loss: 0.3795 Train R²: 0.6295 Test Loss: 0.3583 Test R²: 0.6069\n",
      "154 Train Loss: 0.3780 Train R²: 0.6267 Test Loss: 0.3218 Test R²: 0.6470\n",
      "155 Train Loss: 0.3837 Train R²: 0.6228 Test Loss: 0.3960 Test R²: 0.5656\n",
      "156 Train Loss: 0.4845 Train R²: 0.5231 Test Loss: 0.4704 Test R²: 0.4840\n",
      "157 Train Loss: 0.4475 Train R²: 0.5621 Test Loss: 0.5881 Test R²: 0.3548\n",
      "158 Train Loss: 0.4182 Train R²: 0.5901 Test Loss: 0.3727 Test R²: 0.5911\n",
      "159 Train Loss: 0.4329 Train R²: 0.5762 Test Loss: 0.3729 Test R²: 0.5909\n",
      "160 Train Loss: 0.4568 Train R²: 0.5539 Test Loss: 0.4663 Test R²: 0.4885\n",
      "161 Train Loss: 0.5921 Train R²: 0.4189 Test Loss: 0.7208 Test R²: 0.2092\n",
      "162 Train Loss: 0.4440 Train R²: 0.5646 Test Loss: 0.3487 Test R²: 0.6174\n",
      "163 Train Loss: 0.4704 Train R²: 0.5368 Test Loss: 0.3233 Test R²: 0.6453\n",
      "164 Train Loss: 0.4363 Train R²: 0.5730 Test Loss: 0.3462 Test R²: 0.6202\n",
      "165 Train Loss: 0.4226 Train R²: 0.5844 Test Loss: 0.3207 Test R²: 0.6481\n",
      "166 Train Loss: 0.3964 Train R²: 0.6109 Test Loss: 0.3151 Test R²: 0.6543\n",
      "167 Train Loss: 0.4292 Train R²: 0.5842 Test Loss: 0.3277 Test R²: 0.6405\n",
      "168 Train Loss: 0.4350 Train R²: 0.5732 Test Loss: 0.3362 Test R²: 0.6312\n",
      "169 Train Loss: 0.4128 Train R²: 0.5981 Test Loss: 0.3756 Test R²: 0.5880\n",
      "170 Train Loss: 0.3968 Train R²: 0.6109 Test Loss: 0.3504 Test R²: 0.6156\n",
      "171 Train Loss: 0.4142 Train R²: 0.5936 Test Loss: 0.3135 Test R²: 0.6561\n",
      "172 Train Loss: 0.4057 Train R²: 0.6021 Test Loss: 0.3497 Test R²: 0.6163\n",
      "173 Train Loss: 0.4016 Train R²: 0.6065 Test Loss: 0.5134 Test R²: 0.4367\n",
      "174 Train Loss: 0.4554 Train R²: 0.5525 Test Loss: 0.3420 Test R²: 0.6249\n",
      "175 Train Loss: 0.4548 Train R²: 0.5547 Test Loss: 0.4180 Test R²: 0.5414\n",
      "176 Train Loss: 0.3932 Train R²: 0.6149 Test Loss: 0.3247 Test R²: 0.6438\n",
      "177 Train Loss: 0.3849 Train R²: 0.6216 Test Loss: 0.3319 Test R²: 0.6359\n",
      "178 Train Loss: 0.4293 Train R²: 0.5810 Test Loss: 0.3659 Test R²: 0.5986\n",
      "179 Train Loss: 0.3942 Train R²: 0.6152 Test Loss: 0.2964 Test R²: 0.6748\n",
      "180 Train Loss: 0.4851 Train R²: 0.5331 Test Loss: 0.4104 Test R²: 0.5498\n",
      "181 Train Loss: 0.4489 Train R²: 0.5608 Test Loss: 0.3228 Test R²: 0.6459\n",
      "182 Train Loss: 0.3891 Train R²: 0.6208 Test Loss: 0.3367 Test R²: 0.6306\n",
      "183 Train Loss: 0.3882 Train R²: 0.6230 Test Loss: 0.3226 Test R²: 0.6461\n",
      "184 Train Loss: 0.3736 Train R²: 0.6370 Test Loss: 0.3569 Test R²: 0.6085\n",
      "185 Train Loss: 0.3432 Train R²: 0.6644 Test Loss: 0.3337 Test R²: 0.6339\n",
      "186 Train Loss: 0.3964 Train R²: 0.6100 Test Loss: 0.3609 Test R²: 0.6041\n",
      "187 Train Loss: 0.4761 Train R²: 0.5445 Test Loss: 0.3630 Test R²: 0.6018\n",
      "188 Train Loss: 0.4151 Train R²: 0.5927 Test Loss: 0.4348 Test R²: 0.5230\n",
      "189 Train Loss: 0.4179 Train R²: 0.5901 Test Loss: 0.3592 Test R²: 0.6059\n",
      "190 Train Loss: 0.3334 Train R²: 0.6739 Test Loss: 0.3967 Test R²: 0.5648\n",
      "191 Train Loss: 0.3931 Train R²: 0.6160 Test Loss: 0.3258 Test R²: 0.6425\n",
      "192 Train Loss: 0.3584 Train R²: 0.6481 Test Loss: 0.3179 Test R²: 0.6512\n",
      "193 Train Loss: 0.3819 Train R²: 0.6272 Test Loss: 0.3734 Test R²: 0.5904\n",
      "194 Train Loss: 0.4491 Train R²: 0.5615 Test Loss: 0.3474 Test R²: 0.6189\n",
      "195 Train Loss: 0.3614 Train R²: 0.6472 Test Loss: 0.3115 Test R²: 0.6582\n",
      "196 Train Loss: 0.3644 Train R²: 0.6428 Test Loss: 0.3313 Test R²: 0.6365\n",
      "197 Train Loss: 0.3302 Train R²: 0.6776 Test Loss: 0.3642 Test R²: 0.6005\n",
      "198 Train Loss: 0.4108 Train R²: 0.5963 Test Loss: 0.3400 Test R²: 0.6270\n",
      "199 Train Loss: 0.3908 Train R²: 0.6153 Test Loss: 0.3150 Test R²: 0.6544\n",
      "200 Train Loss: 0.3678 Train R²: 0.6430 Test Loss: 0.3147 Test R²: 0.6547\n",
      "201 Train Loss: 0.3728 Train R²: 0.6330 Test Loss: 0.3871 Test R²: 0.5753\n",
      "202 Train Loss: 0.3893 Train R²: 0.6176 Test Loss: 0.3516 Test R²: 0.6143\n",
      "203 Train Loss: 0.3674 Train R²: 0.6415 Test Loss: 0.3834 Test R²: 0.5794\n",
      "204 Train Loss: 0.3317 Train R²: 0.6761 Test Loss: 0.4002 Test R²: 0.5609\n",
      "205 Train Loss: 0.3796 Train R²: 0.6308 Test Loss: 0.3690 Test R²: 0.5952\n",
      "206 Train Loss: 0.3701 Train R²: 0.6374 Test Loss: 0.3673 Test R²: 0.5970\n",
      "207 Train Loss: 0.3716 Train R²: 0.6337 Test Loss: 0.3947 Test R²: 0.5670\n",
      "208 Train Loss: 0.3298 Train R²: 0.6759 Test Loss: 0.3507 Test R²: 0.6152\n",
      "209 Train Loss: 0.3290 Train R²: 0.6787 Test Loss: 0.3173 Test R²: 0.6519\n",
      "210 Train Loss: 0.3083 Train R²: 0.6979 Test Loss: 0.3620 Test R²: 0.6028\n",
      "211 Train Loss: 0.3580 Train R²: 0.6501 Test Loss: 0.3188 Test R²: 0.6503\n",
      "212 Train Loss: 0.4216 Train R²: 0.5904 Test Loss: 0.3648 Test R²: 0.5997\n",
      "213 Train Loss: 0.3692 Train R²: 0.6365 Test Loss: 0.3352 Test R²: 0.6322\n",
      "214 Train Loss: 0.3425 Train R²: 0.6662 Test Loss: 0.3710 Test R²: 0.5929\n",
      "215 Train Loss: 0.3698 Train R²: 0.6390 Test Loss: 0.3236 Test R²: 0.6450\n",
      "216 Train Loss: 0.3443 Train R²: 0.6646 Test Loss: 0.3797 Test R²: 0.5835\n",
      "217 Train Loss: 0.3376 Train R²: 0.6702 Test Loss: 0.4411 Test R²: 0.5161\n",
      "218 Train Loss: 0.3466 Train R²: 0.6614 Test Loss: 0.4343 Test R²: 0.5236\n",
      "219 Train Loss: 0.3978 Train R²: 0.6088 Test Loss: 0.3031 Test R²: 0.6675\n",
      "220 Train Loss: 0.3277 Train R²: 0.6789 Test Loss: 0.3590 Test R²: 0.6061\n",
      "221 Train Loss: 0.3183 Train R²: 0.6890 Test Loss: 0.3274 Test R²: 0.6409\n",
      "222 Train Loss: 0.3560 Train R²: 0.6508 Test Loss: 0.3235 Test R²: 0.6451\n",
      "223 Train Loss: 0.3391 Train R²: 0.6688 Test Loss: 0.3394 Test R²: 0.6276\n",
      "224 Train Loss: 0.4401 Train R²: 0.5695 Test Loss: 0.3511 Test R²: 0.6148\n",
      "225 Train Loss: 0.3611 Train R²: 0.6471 Test Loss: 0.3354 Test R²: 0.6320\n",
      "226 Train Loss: 0.4712 Train R²: 0.5388 Test Loss: 0.3769 Test R²: 0.5866\n",
      "227 Train Loss: 0.3673 Train R²: 0.6401 Test Loss: 0.3340 Test R²: 0.6336\n",
      "228 Train Loss: 0.3349 Train R²: 0.6728 Test Loss: 0.3538 Test R²: 0.6118\n",
      "229 Train Loss: 0.3019 Train R²: 0.7029 Test Loss: 0.3273 Test R²: 0.6410\n",
      "230 Train Loss: 0.3268 Train R²: 0.6820 Test Loss: 0.3252 Test R²: 0.6432\n",
      "231 Train Loss: 0.3581 Train R²: 0.6458 Test Loss: 0.3130 Test R²: 0.6566\n",
      "232 Train Loss: 0.3740 Train R²: 0.6368 Test Loss: 0.3167 Test R²: 0.6526\n",
      "233 Train Loss: 0.3005 Train R²: 0.7086 Test Loss: 0.3887 Test R²: 0.5736\n",
      "234 Train Loss: 0.3779 Train R²: 0.6346 Test Loss: 0.3897 Test R²: 0.5725\n",
      "235 Train Loss: 0.3386 Train R²: 0.6697 Test Loss: 0.3419 Test R²: 0.6249\n",
      "236 Train Loss: 0.2988 Train R²: 0.7085 Test Loss: 0.3376 Test R²: 0.6296\n",
      "237 Train Loss: 0.3145 Train R²: 0.6892 Test Loss: 0.3467 Test R²: 0.6196\n",
      "238 Train Loss: 0.3446 Train R²: 0.6624 Test Loss: 0.3220 Test R²: 0.6467\n",
      "239 Train Loss: 0.3263 Train R²: 0.6824 Test Loss: 0.3269 Test R²: 0.6414\n",
      "240 Train Loss: 0.3913 Train R²: 0.6161 Test Loss: 0.4600 Test R²: 0.4954\n",
      "241 Train Loss: 0.3735 Train R²: 0.6335 Test Loss: 0.3110 Test R²: 0.6588\n",
      "242 Train Loss: 0.3707 Train R²: 0.6399 Test Loss: 0.3408 Test R²: 0.6261\n",
      "243 Train Loss: 0.3360 Train R²: 0.6691 Test Loss: 0.3273 Test R²: 0.6410\n",
      "244 Train Loss: 0.3758 Train R²: 0.6324 Test Loss: 0.3398 Test R²: 0.6272\n",
      "245 Train Loss: 0.3450 Train R²: 0.6609 Test Loss: 0.3135 Test R²: 0.6561\n",
      "246 Train Loss: 0.4272 Train R²: 0.5794 Test Loss: 0.3639 Test R²: 0.6008\n",
      "247 Train Loss: 0.3439 Train R²: 0.6636 Test Loss: 0.3517 Test R²: 0.6142\n",
      "248 Train Loss: 0.3967 Train R²: 0.6170 Test Loss: 0.3372 Test R²: 0.6301\n",
      "249 Train Loss: 0.4066 Train R²: 0.6011 Test Loss: 0.3762 Test R²: 0.5873\n",
      "250 Train Loss: 0.3657 Train R²: 0.6416 Test Loss: 0.3614 Test R²: 0.6035\n",
      "251 Train Loss: 0.3463 Train R²: 0.6650 Test Loss: 0.3325 Test R²: 0.6353\n",
      "252 Train Loss: 0.3610 Train R²: 0.6443 Test Loss: 0.3130 Test R²: 0.6566\n",
      "253 Train Loss: 0.3422 Train R²: 0.6662 Test Loss: 0.4135 Test R²: 0.5464\n",
      "254 Train Loss: 0.4016 Train R²: 0.6087 Test Loss: 0.3890 Test R²: 0.5733\n",
      "255 Train Loss: 0.3345 Train R²: 0.6714 Test Loss: 0.3710 Test R²: 0.5930\n",
      "256 Train Loss: 0.3123 Train R²: 0.6929 Test Loss: 0.3253 Test R²: 0.6432\n",
      "257 Train Loss: 0.3829 Train R²: 0.6241 Test Loss: 0.3985 Test R²: 0.5628\n",
      "258 Train Loss: 0.3221 Train R²: 0.6832 Test Loss: 0.3283 Test R²: 0.6399\n",
      "259 Train Loss: 0.3511 Train R²: 0.6570 Test Loss: 0.3138 Test R²: 0.6558\n",
      "260 Train Loss: 0.3249 Train R²: 0.6803 Test Loss: 0.3619 Test R²: 0.6030\n",
      "261 Train Loss: 0.3023 Train R²: 0.7025 Test Loss: 0.3406 Test R²: 0.6264\n",
      "262 Train Loss: 0.3400 Train R²: 0.6688 Test Loss: 0.3639 Test R²: 0.6008\n",
      "263 Train Loss: 0.3656 Train R²: 0.6414 Test Loss: 0.3915 Test R²: 0.5705\n",
      "264 Train Loss: 0.3757 Train R²: 0.6321 Test Loss: 0.3199 Test R²: 0.6491\n",
      "265 Train Loss: 0.2995 Train R²: 0.7079 Test Loss: 0.3154 Test R²: 0.6540\n",
      "266 Train Loss: 0.2523 Train R²: 0.7526 Test Loss: 0.3020 Test R²: 0.6687\n",
      "267 Train Loss: 0.2711 Train R²: 0.7347 Test Loss: 0.3060 Test R²: 0.6643\n",
      "268 Train Loss: 0.3229 Train R²: 0.6843 Test Loss: 0.2976 Test R²: 0.6736\n",
      "269 Train Loss: 0.2931 Train R²: 0.7117 Test Loss: 0.3360 Test R²: 0.6314\n",
      "270 Train Loss: 0.3048 Train R²: 0.7024 Test Loss: 0.3468 Test R²: 0.6196\n",
      "271 Train Loss: 0.2902 Train R²: 0.7186 Test Loss: 0.3374 Test R²: 0.6298\n",
      "272 Train Loss: 0.3135 Train R²: 0.6944 Test Loss: 0.3608 Test R²: 0.6041\n",
      "273 Train Loss: 0.3225 Train R²: 0.6871 Test Loss: 0.3679 Test R²: 0.5964\n",
      "274 Train Loss: 0.2925 Train R²: 0.7127 Test Loss: 0.4440 Test R²: 0.5129\n",
      "275 Train Loss: 0.3257 Train R²: 0.6818 Test Loss: 0.3407 Test R²: 0.6263\n",
      "276 Train Loss: 0.2945 Train R²: 0.7128 Test Loss: 0.3130 Test R²: 0.6566\n",
      "277 Train Loss: 0.3463 Train R²: 0.6604 Test Loss: 0.3294 Test R²: 0.6386\n",
      "278 Train Loss: 0.3251 Train R²: 0.6797 Test Loss: 0.3684 Test R²: 0.5958\n",
      "279 Train Loss: 0.2842 Train R²: 0.7208 Test Loss: 0.3199 Test R²: 0.6490\n",
      "280 Train Loss: 0.2484 Train R²: 0.7568 Test Loss: 0.3170 Test R²: 0.6523\n",
      "281 Train Loss: 0.2765 Train R²: 0.7283 Test Loss: 0.3780 Test R²: 0.5854\n",
      "282 Train Loss: 0.2766 Train R²: 0.7282 Test Loss: 0.3630 Test R²: 0.6018\n",
      "283 Train Loss: 0.2645 Train R²: 0.7409 Test Loss: 0.3098 Test R²: 0.6601\n",
      "284 Train Loss: 0.2819 Train R²: 0.7256 Test Loss: 0.3811 Test R²: 0.5820\n",
      "285 Train Loss: 0.2981 Train R²: 0.7086 Test Loss: 0.3578 Test R²: 0.6075\n",
      "286 Train Loss: 0.2858 Train R²: 0.7215 Test Loss: 0.3773 Test R²: 0.5861\n",
      "287 Train Loss: 0.3062 Train R²: 0.6984 Test Loss: 0.3322 Test R²: 0.6355\n",
      "288 Train Loss: 0.3108 Train R²: 0.6950 Test Loss: 0.3452 Test R²: 0.6212\n",
      "289 Train Loss: 0.2795 Train R²: 0.7253 Test Loss: 0.4621 Test R²: 0.4931\n",
      "290 Train Loss: 0.3055 Train R²: 0.7006 Test Loss: 0.3040 Test R²: 0.6664\n",
      "291 Train Loss: 0.3165 Train R²: 0.6887 Test Loss: 0.3190 Test R²: 0.6500\n",
      "292 Train Loss: 0.2922 Train R²: 0.7133 Test Loss: 0.3127 Test R²: 0.6570\n",
      "293 Train Loss: 0.2779 Train R²: 0.7286 Test Loss: 0.3184 Test R²: 0.6507\n",
      "294 Train Loss: 0.2749 Train R²: 0.7287 Test Loss: 0.3101 Test R²: 0.6598\n",
      "295 Train Loss: 0.2772 Train R²: 0.7275 Test Loss: 0.3471 Test R²: 0.6192\n",
      "296 Train Loss: 0.3494 Train R²: 0.6568 Test Loss: 0.3812 Test R²: 0.5818\n",
      "297 Train Loss: 0.3259 Train R²: 0.6792 Test Loss: 0.3301 Test R²: 0.6378\n",
      "298 Train Loss: 0.2691 Train R²: 0.7365 Test Loss: 0.4706 Test R²: 0.4837\n",
      "299 Train Loss: 0.2990 Train R²: 0.7083 Test Loss: 0.3112 Test R²: 0.6586\n",
      "300 Train Loss: 0.2741 Train R²: 0.7294 Test Loss: 0.3041 Test R²: 0.6664\n",
      "301 Train Loss: 0.3081 Train R²: 0.6962 Test Loss: 0.3134 Test R²: 0.6562\n",
      "302 Train Loss: 0.2986 Train R²: 0.7069 Test Loss: 0.3585 Test R²: 0.6067\n",
      "303 Train Loss: 0.2719 Train R²: 0.7346 Test Loss: 0.4625 Test R²: 0.4926\n",
      "304 Train Loss: 0.3411 Train R²: 0.6649 Test Loss: 0.3010 Test R²: 0.6698\n",
      "305 Train Loss: 0.3070 Train R²: 0.7011 Test Loss: 0.3128 Test R²: 0.6568\n",
      "306 Train Loss: 0.2964 Train R²: 0.7074 Test Loss: 0.3224 Test R²: 0.6463\n",
      "307 Train Loss: 0.2659 Train R²: 0.7396 Test Loss: 0.3253 Test R²: 0.6431\n",
      "308 Train Loss: 0.2892 Train R²: 0.7163 Test Loss: 0.3107 Test R²: 0.6591\n",
      "309 Train Loss: 0.2793 Train R²: 0.7250 Test Loss: 0.3636 Test R²: 0.6011\n",
      "310 Train Loss: 0.2746 Train R²: 0.7296 Test Loss: 0.3294 Test R²: 0.6387\n",
      "311 Train Loss: 0.2565 Train R²: 0.7489 Test Loss: 0.2993 Test R²: 0.6716\n",
      "312 Train Loss: 0.2731 Train R²: 0.7367 Test Loss: 0.3482 Test R²: 0.6180\n",
      "313 Train Loss: 0.2740 Train R²: 0.7316 Test Loss: 0.3297 Test R²: 0.6383\n",
      "314 Train Loss: 0.2480 Train R²: 0.7560 Test Loss: 0.3140 Test R²: 0.6555\n",
      "315 Train Loss: 0.2602 Train R²: 0.7474 Test Loss: 0.3016 Test R²: 0.6691\n",
      "316 Train Loss: 0.2460 Train R²: 0.7603 Test Loss: 0.3083 Test R²: 0.6618\n",
      "317 Train Loss: 0.3133 Train R²: 0.6925 Test Loss: 0.3409 Test R²: 0.6260\n",
      "318 Train Loss: 0.2562 Train R²: 0.7492 Test Loss: 0.4457 Test R²: 0.5110\n",
      "319 Train Loss: 0.2904 Train R²: 0.7136 Test Loss: 0.3260 Test R²: 0.6424\n",
      "320 Train Loss: 0.3118 Train R²: 0.6958 Test Loss: 0.3524 Test R²: 0.6134\n",
      "321 Train Loss: 0.2742 Train R²: 0.7306 Test Loss: 0.3115 Test R²: 0.6583\n",
      "322 Train Loss: 0.2468 Train R²: 0.7605 Test Loss: 0.3426 Test R²: 0.6242\n",
      "323 Train Loss: 0.2576 Train R²: 0.7471 Test Loss: 0.3125 Test R²: 0.6572\n",
      "324 Train Loss: 0.2556 Train R²: 0.7500 Test Loss: 0.3146 Test R²: 0.6548\n",
      "325 Train Loss: 0.3197 Train R²: 0.6872 Test Loss: 0.3134 Test R²: 0.6561\n",
      "326 Train Loss: 0.2785 Train R²: 0.7264 Test Loss: 0.3455 Test R²: 0.6209\n",
      "327 Train Loss: 0.3117 Train R²: 0.6938 Test Loss: 0.3045 Test R²: 0.6660\n",
      "328 Train Loss: 0.3164 Train R²: 0.6930 Test Loss: 0.3259 Test R²: 0.6425\n",
      "329 Train Loss: 0.2508 Train R²: 0.7548 Test Loss: 0.3232 Test R²: 0.6455\n",
      "330 Train Loss: 0.2672 Train R²: 0.7402 Test Loss: 0.3212 Test R²: 0.6476\n",
      "331 Train Loss: 0.2436 Train R²: 0.7619 Test Loss: 0.3559 Test R²: 0.6096\n",
      "332 Train Loss: 0.2489 Train R²: 0.7564 Test Loss: 0.3640 Test R²: 0.6007\n",
      "333 Train Loss: 0.3132 Train R²: 0.6936 Test Loss: 0.3183 Test R²: 0.6508\n",
      "334 Train Loss: 0.3298 Train R²: 0.6766 Test Loss: 0.3389 Test R²: 0.6282\n",
      "335 Train Loss: 0.3005 Train R²: 0.7069 Test Loss: 0.3621 Test R²: 0.6028\n",
      "336 Train Loss: 0.2602 Train R²: 0.7459 Test Loss: 0.3135 Test R²: 0.6560\n",
      "337 Train Loss: 0.2444 Train R²: 0.7608 Test Loss: 0.3082 Test R²: 0.6619\n",
      "338 Train Loss: 0.2980 Train R²: 0.7103 Test Loss: 0.3535 Test R²: 0.6122\n",
      "339 Train Loss: 0.3085 Train R²: 0.7022 Test Loss: 0.3868 Test R²: 0.5756\n",
      "340 Train Loss: 0.2967 Train R²: 0.7087 Test Loss: 0.3355 Test R²: 0.6320\n",
      "341 Train Loss: 0.3256 Train R²: 0.6826 Test Loss: 0.3577 Test R²: 0.6076\n",
      "342 Train Loss: 0.2380 Train R²: 0.7662 Test Loss: 0.3210 Test R²: 0.6479\n",
      "343 Train Loss: 0.2895 Train R²: 0.7154 Test Loss: 0.3280 Test R²: 0.6402\n",
      "344 Train Loss: 0.2749 Train R²: 0.7299 Test Loss: 0.3334 Test R²: 0.6342\n",
      "345 Train Loss: 0.2732 Train R²: 0.7364 Test Loss: 0.3642 Test R²: 0.6005\n",
      "346 Train Loss: 0.2661 Train R²: 0.7395 Test Loss: 0.3261 Test R²: 0.6422\n",
      "347 Train Loss: 0.2220 Train R²: 0.7812 Test Loss: 0.3341 Test R²: 0.6334\n",
      "348 Train Loss: 0.2749 Train R²: 0.7290 Test Loss: 0.3170 Test R²: 0.6523\n",
      "349 Train Loss: 0.2827 Train R²: 0.7221 Test Loss: 0.3310 Test R²: 0.6369\n",
      "350 Train Loss: 0.2594 Train R²: 0.7450 Test Loss: 0.3056 Test R²: 0.6648\n",
      "351 Train Loss: 0.2496 Train R²: 0.7543 Test Loss: 0.3148 Test R²: 0.6546\n",
      "352 Train Loss: 0.2804 Train R²: 0.7233 Test Loss: 0.3098 Test R²: 0.6601\n",
      "353 Train Loss: 0.2441 Train R²: 0.7589 Test Loss: 0.3295 Test R²: 0.6385\n",
      "354 Train Loss: 0.2746 Train R²: 0.7299 Test Loss: 0.3231 Test R²: 0.6455\n",
      "355 Train Loss: 0.2686 Train R²: 0.7379 Test Loss: 0.3680 Test R²: 0.5962\n",
      "356 Train Loss: 0.2506 Train R²: 0.7543 Test Loss: 0.4013 Test R²: 0.5598\n",
      "357 Train Loss: 0.2399 Train R²: 0.7631 Test Loss: 0.3204 Test R²: 0.6485\n",
      "358 Train Loss: 0.2364 Train R²: 0.7690 Test Loss: 0.3296 Test R²: 0.6384\n",
      "359 Train Loss: 0.3103 Train R²: 0.6963 Test Loss: 0.3243 Test R²: 0.6442\n",
      "360 Train Loss: 0.2569 Train R²: 0.7483 Test Loss: 0.3220 Test R²: 0.6467\n",
      "361 Train Loss: 0.2496 Train R²: 0.7578 Test Loss: 0.3413 Test R²: 0.6256\n",
      "362 Train Loss: 0.2310 Train R²: 0.7750 Test Loss: 0.3290 Test R²: 0.6390\n",
      "363 Train Loss: 0.2412 Train R²: 0.7627 Test Loss: 0.3228 Test R²: 0.6458\n",
      "364 Train Loss: 0.2448 Train R²: 0.7602 Test Loss: 0.3965 Test R²: 0.5651\n",
      "365 Train Loss: 0.2652 Train R²: 0.7408 Test Loss: 0.3234 Test R²: 0.6453\n",
      "366 Train Loss: 0.2643 Train R²: 0.7400 Test Loss: 0.3407 Test R²: 0.6262\n",
      "367 Train Loss: 0.2142 Train R²: 0.7920 Test Loss: 0.3217 Test R²: 0.6471\n",
      "368 Train Loss: 0.2290 Train R²: 0.7754 Test Loss: 0.3167 Test R²: 0.6526\n",
      "369 Train Loss: 0.2308 Train R²: 0.7743 Test Loss: 0.3172 Test R²: 0.6520\n",
      "370 Train Loss: 0.2856 Train R²: 0.7202 Test Loss: 0.3058 Test R²: 0.6645\n",
      "371 Train Loss: 0.2466 Train R²: 0.7618 Test Loss: 0.3221 Test R²: 0.6466\n",
      "372 Train Loss: 0.2508 Train R²: 0.7551 Test Loss: 0.3191 Test R²: 0.6499\n",
      "373 Train Loss: 0.2084 Train R²: 0.7953 Test Loss: 0.3021 Test R²: 0.6686\n",
      "374 Train Loss: 0.2022 Train R²: 0.8018 Test Loss: 0.3015 Test R²: 0.6693\n",
      "375 Train Loss: 0.3335 Train R²: 0.6754 Test Loss: 0.3088 Test R²: 0.6612\n",
      "376 Train Loss: 0.2241 Train R²: 0.7825 Test Loss: 0.3356 Test R²: 0.6319\n",
      "377 Train Loss: 0.3092 Train R²: 0.7038 Test Loss: 0.3697 Test R²: 0.5945\n",
      "378 Train Loss: 0.2686 Train R²: 0.7383 Test Loss: 0.3479 Test R²: 0.6184\n",
      "379 Train Loss: 0.2799 Train R²: 0.7245 Test Loss: 0.3227 Test R²: 0.6460\n",
      "380 Train Loss: 0.3000 Train R²: 0.7049 Test Loss: 0.3194 Test R²: 0.6496\n",
      "381 Train Loss: 0.2123 Train R²: 0.7915 Test Loss: 0.3223 Test R²: 0.6464\n",
      "382 Train Loss: 0.2251 Train R²: 0.7775 Test Loss: 0.3289 Test R²: 0.6392\n",
      "383 Train Loss: 0.2418 Train R²: 0.7628 Test Loss: 0.2965 Test R²: 0.6747\n",
      "384 Train Loss: 0.2113 Train R²: 0.7923 Test Loss: 0.3120 Test R²: 0.6577\n",
      "385 Train Loss: 0.1991 Train R²: 0.8062 Test Loss: 0.3020 Test R²: 0.6687\n",
      "386 Train Loss: 0.2460 Train R²: 0.7596 Test Loss: 0.3088 Test R²: 0.6612\n",
      "387 Train Loss: 0.2635 Train R²: 0.7412 Test Loss: 0.2944 Test R²: 0.6770\n",
      "388 Train Loss: 0.2326 Train R²: 0.7717 Test Loss: 0.3278 Test R²: 0.6403\n",
      "389 Train Loss: 0.2382 Train R²: 0.7653 Test Loss: 0.3115 Test R²: 0.6583\n",
      "390 Train Loss: 0.2173 Train R²: 0.7877 Test Loss: 0.3033 Test R²: 0.6672\n",
      "391 Train Loss: 0.2364 Train R²: 0.7686 Test Loss: 0.3422 Test R²: 0.6246\n",
      "392 Train Loss: 0.2325 Train R²: 0.7723 Test Loss: 0.3600 Test R²: 0.6050\n",
      "393 Train Loss: 0.2708 Train R²: 0.7386 Test Loss: 0.3177 Test R²: 0.6515\n",
      "394 Train Loss: 0.2559 Train R²: 0.7494 Test Loss: 0.3165 Test R²: 0.6528\n",
      "395 Train Loss: 0.2286 Train R²: 0.7768 Test Loss: 0.3225 Test R²: 0.6462\n",
      "396 Train Loss: 0.2262 Train R²: 0.7777 Test Loss: 0.3131 Test R²: 0.6566\n",
      "397 Train Loss: 0.2037 Train R²: 0.7993 Test Loss: 0.3429 Test R²: 0.6239\n",
      "398 Train Loss: 0.2316 Train R²: 0.7733 Test Loss: 0.3161 Test R²: 0.6532\n",
      "399 Train Loss: 0.2383 Train R²: 0.7716 Test Loss: 0.3033 Test R²: 0.6672\n",
      "400 Train Loss: 0.2363 Train R²: 0.7686 Test Loss: 0.3070 Test R²: 0.6632\n",
      "401 Train Loss: 0.2141 Train R²: 0.7901 Test Loss: 0.3056 Test R²: 0.6647\n",
      "402 Train Loss: 0.2218 Train R²: 0.7870 Test Loss: 0.3035 Test R²: 0.6671\n",
      "403 Train Loss: 0.2170 Train R²: 0.7869 Test Loss: 0.3137 Test R²: 0.6559\n",
      "404 Train Loss: 0.2000 Train R²: 0.8086 Test Loss: 0.3161 Test R²: 0.6532\n",
      "405 Train Loss: 0.2299 Train R²: 0.7742 Test Loss: 0.2948 Test R²: 0.6766\n",
      "406 Train Loss: 0.2527 Train R²: 0.7522 Test Loss: 0.2960 Test R²: 0.6752\n",
      "407 Train Loss: 0.2125 Train R²: 0.7905 Test Loss: 0.2950 Test R²: 0.6764\n",
      "408 Train Loss: 0.2112 Train R²: 0.7932 Test Loss: 0.3265 Test R²: 0.6419\n",
      "409 Train Loss: 0.2748 Train R²: 0.7321 Test Loss: 0.3393 Test R²: 0.6278\n",
      "410 Train Loss: 0.2646 Train R²: 0.7394 Test Loss: 0.3297 Test R²: 0.6383\n",
      "411 Train Loss: 0.2562 Train R²: 0.7489 Test Loss: 0.3405 Test R²: 0.6264\n",
      "412 Train Loss: 0.2242 Train R²: 0.7817 Test Loss: 0.3497 Test R²: 0.6164\n",
      "413 Train Loss: 0.2331 Train R²: 0.7719 Test Loss: 0.3110 Test R²: 0.6588\n",
      "414 Train Loss: 0.2428 Train R²: 0.7603 Test Loss: 0.2977 Test R²: 0.6734\n",
      "415 Train Loss: 0.2250 Train R²: 0.7786 Test Loss: 0.2904 Test R²: 0.6815\n",
      "416 Train Loss: 0.2192 Train R²: 0.7845 Test Loss: 0.2930 Test R²: 0.6785\n",
      "417 Train Loss: 0.2282 Train R²: 0.7747 Test Loss: 0.2966 Test R²: 0.6746\n",
      "418 Train Loss: 0.2248 Train R²: 0.7794 Test Loss: 0.3127 Test R²: 0.6569\n",
      "419 Train Loss: 0.2075 Train R²: 0.7961 Test Loss: 0.3126 Test R²: 0.6570\n",
      "420 Train Loss: 0.2323 Train R²: 0.7710 Test Loss: 0.3045 Test R²: 0.6660\n",
      "421 Train Loss: 0.2101 Train R²: 0.7928 Test Loss: 0.3185 Test R²: 0.6506\n",
      "422 Train Loss: 0.2508 Train R²: 0.7565 Test Loss: 0.3309 Test R²: 0.6370\n",
      "423 Train Loss: 0.1981 Train R²: 0.8051 Test Loss: 0.3276 Test R²: 0.6406\n",
      "424 Train Loss: 0.2167 Train R²: 0.7888 Test Loss: 0.3267 Test R²: 0.6415\n",
      "425 Train Loss: 0.2388 Train R²: 0.7663 Test Loss: 0.2945 Test R²: 0.6770\n",
      "426 Train Loss: 0.2226 Train R²: 0.7821 Test Loss: 0.2986 Test R²: 0.6724\n",
      "427 Train Loss: 0.2429 Train R²: 0.7613 Test Loss: 0.3454 Test R²: 0.6211\n",
      "428 Train Loss: 0.2339 Train R²: 0.7707 Test Loss: 0.3042 Test R²: 0.6663\n",
      "429 Train Loss: 0.2206 Train R²: 0.7836 Test Loss: 0.3087 Test R²: 0.6614\n",
      "430 Train Loss: 0.2241 Train R²: 0.7823 Test Loss: 0.3159 Test R²: 0.6535\n",
      "431 Train Loss: 0.1951 Train R²: 0.8101 Test Loss: 0.3244 Test R²: 0.6442\n",
      "432 Train Loss: 0.1991 Train R²: 0.8047 Test Loss: 0.3182 Test R²: 0.6509\n",
      "433 Train Loss: 0.2662 Train R²: 0.7408 Test Loss: 0.3418 Test R²: 0.6250\n",
      "434 Train Loss: 0.2786 Train R²: 0.7249 Test Loss: 0.2980 Test R²: 0.6731\n",
      "435 Train Loss: 0.2359 Train R²: 0.7689 Test Loss: 0.2998 Test R²: 0.6711\n",
      "436 Train Loss: 0.2073 Train R²: 0.7962 Test Loss: 0.3282 Test R²: 0.6400\n",
      "437 Train Loss: 0.1916 Train R²: 0.8124 Test Loss: 0.3040 Test R²: 0.6665\n",
      "438 Train Loss: 0.2340 Train R²: 0.7728 Test Loss: 0.3122 Test R²: 0.6575\n",
      "439 Train Loss: 0.3042 Train R²: 0.7027 Test Loss: 0.3038 Test R²: 0.6667\n",
      "440 Train Loss: 0.2001 Train R²: 0.8041 Test Loss: 0.2862 Test R²: 0.6860\n",
      "441 Train Loss: 0.2672 Train R²: 0.7422 Test Loss: 0.3917 Test R²: 0.5702\n",
      "442 Train Loss: 0.2401 Train R²: 0.7637 Test Loss: 0.3509 Test R²: 0.6150\n",
      "443 Train Loss: 0.2717 Train R²: 0.7326 Test Loss: 0.3228 Test R²: 0.6459\n",
      "444 Train Loss: 0.2248 Train R²: 0.7826 Test Loss: 0.3086 Test R²: 0.6614\n",
      "445 Train Loss: 0.2238 Train R²: 0.7831 Test Loss: 0.3239 Test R²: 0.6446\n",
      "446 Train Loss: 0.2418 Train R²: 0.7630 Test Loss: 0.3060 Test R²: 0.6643\n",
      "447 Train Loss: 0.2506 Train R²: 0.7529 Test Loss: 0.2942 Test R²: 0.6772\n",
      "448 Train Loss: 0.2713 Train R²: 0.7335 Test Loss: 0.3148 Test R²: 0.6547\n",
      "449 Train Loss: 0.1836 Train R²: 0.8213 Test Loss: 0.3141 Test R²: 0.6554\n",
      "450 Train Loss: 0.2038 Train R²: 0.7991 Test Loss: 0.3427 Test R²: 0.6241\n",
      "451 Train Loss: 0.2213 Train R²: 0.7850 Test Loss: 0.3183 Test R²: 0.6509\n",
      "452 Train Loss: 0.2371 Train R²: 0.7686 Test Loss: 0.3071 Test R²: 0.6631\n",
      "453 Train Loss: 0.2180 Train R²: 0.7877 Test Loss: 0.2939 Test R²: 0.6776\n",
      "454 Train Loss: 0.1987 Train R²: 0.8051 Test Loss: 0.2988 Test R²: 0.6722\n",
      "455 Train Loss: 0.2739 Train R²: 0.7301 Test Loss: 0.2898 Test R²: 0.6821\n",
      "456 Train Loss: 0.2611 Train R²: 0.7431 Test Loss: 0.2742 Test R²: 0.6992\n",
      "457 Train Loss: 0.2285 Train R²: 0.7751 Test Loss: 0.2940 Test R²: 0.6774\n",
      "458 Train Loss: 0.2727 Train R²: 0.7309 Test Loss: 0.3051 Test R²: 0.6653\n",
      "459 Train Loss: 0.2105 Train R²: 0.7933 Test Loss: 0.3069 Test R²: 0.6634\n",
      "460 Train Loss: 0.2188 Train R²: 0.7867 Test Loss: 0.2956 Test R²: 0.6757\n",
      "461 Train Loss: 0.2259 Train R²: 0.7816 Test Loss: 0.3106 Test R²: 0.6592\n",
      "462 Train Loss: 0.1911 Train R²: 0.8123 Test Loss: 0.3082 Test R²: 0.6619\n",
      "463 Train Loss: 0.2369 Train R²: 0.7679 Test Loss: 0.3478 Test R²: 0.6184\n",
      "464 Train Loss: 0.2351 Train R²: 0.7702 Test Loss: 0.3005 Test R²: 0.6704\n",
      "465 Train Loss: 0.2349 Train R²: 0.7696 Test Loss: 0.3028 Test R²: 0.6679\n",
      "466 Train Loss: 0.2369 Train R²: 0.7678 Test Loss: 0.3064 Test R²: 0.6638\n",
      "467 Train Loss: 0.2190 Train R²: 0.7851 Test Loss: 0.3278 Test R²: 0.6404\n",
      "468 Train Loss: 0.2005 Train R²: 0.8039 Test Loss: 0.3188 Test R²: 0.6503\n",
      "469 Train Loss: 0.1974 Train R²: 0.8065 Test Loss: 0.3295 Test R²: 0.6385\n",
      "470 Train Loss: 0.1986 Train R²: 0.8063 Test Loss: 0.2989 Test R²: 0.6721\n",
      "471 Train Loss: 0.1854 Train R²: 0.8177 Test Loss: 0.2930 Test R²: 0.6786\n",
      "472 Train Loss: 0.2552 Train R²: 0.7493 Test Loss: 0.3087 Test R²: 0.6613\n",
      "473 Train Loss: 0.2185 Train R²: 0.7869 Test Loss: 0.2995 Test R²: 0.6714\n",
      "474 Train Loss: 0.1709 Train R²: 0.8334 Test Loss: 0.2857 Test R²: 0.6866\n",
      "475 Train Loss: 0.2175 Train R²: 0.7854 Test Loss: 0.3032 Test R²: 0.6674\n",
      "476 Train Loss: 0.2218 Train R²: 0.7821 Test Loss: 0.2900 Test R²: 0.6819\n",
      "477 Train Loss: 0.1840 Train R²: 0.8193 Test Loss: 0.2994 Test R²: 0.6716\n",
      "478 Train Loss: 0.1906 Train R²: 0.8145 Test Loss: 0.3268 Test R²: 0.6415\n",
      "479 Train Loss: 0.2084 Train R²: 0.7963 Test Loss: 0.3411 Test R²: 0.6258\n",
      "480 Train Loss: 0.2351 Train R²: 0.7689 Test Loss: 0.2976 Test R²: 0.6735\n",
      "481 Train Loss: 0.2021 Train R²: 0.8029 Test Loss: 0.3289 Test R²: 0.6392\n",
      "482 Train Loss: 0.2058 Train R²: 0.7988 Test Loss: 0.3768 Test R²: 0.5866\n",
      "483 Train Loss: 0.2358 Train R²: 0.7686 Test Loss: 0.3124 Test R²: 0.6573\n",
      "484 Train Loss: 0.2397 Train R²: 0.7654 Test Loss: 0.3197 Test R²: 0.6493\n",
      "485 Train Loss: 0.2063 Train R²: 0.7974 Test Loss: 0.3063 Test R²: 0.6640\n",
      "486 Train Loss: 0.1808 Train R²: 0.8242 Test Loss: 0.3183 Test R²: 0.6508\n",
      "487 Train Loss: 0.1948 Train R²: 0.8109 Test Loss: 0.2784 Test R²: 0.6946\n",
      "488 Train Loss: 0.1910 Train R²: 0.8146 Test Loss: 0.2950 Test R²: 0.6763\n",
      "489 Train Loss: 0.2194 Train R²: 0.7853 Test Loss: 0.3035 Test R²: 0.6671\n",
      "490 Train Loss: 0.1961 Train R²: 0.8095 Test Loss: 0.3139 Test R²: 0.6556\n",
      "491 Train Loss: 0.2125 Train R²: 0.7919 Test Loss: 0.2942 Test R²: 0.6772\n",
      "492 Train Loss: 0.2385 Train R²: 0.7642 Test Loss: 0.2959 Test R²: 0.6754\n",
      "493 Train Loss: 0.2191 Train R²: 0.7872 Test Loss: 0.2887 Test R²: 0.6833\n",
      "494 Train Loss: 0.2082 Train R²: 0.7971 Test Loss: 0.2954 Test R²: 0.6759\n",
      "495 Train Loss: 0.2022 Train R²: 0.8034 Test Loss: 0.3010 Test R²: 0.6698\n",
      "496 Train Loss: 0.1919 Train R²: 0.8119 Test Loss: 0.2867 Test R²: 0.6855\n",
      "497 Train Loss: 0.2044 Train R²: 0.8011 Test Loss: 0.2996 Test R²: 0.6713\n",
      "498 Train Loss: 0.1662 Train R²: 0.8370 Test Loss: 0.3163 Test R²: 0.6530\n",
      "499 Train Loss: 0.1883 Train R²: 0.8159 Test Loss: 0.2953 Test R²: 0.6761\n",
      "500 Train Loss: 0.2631 Train R²: 0.7418 Test Loss: 0.3202 Test R²: 0.6487\n",
      "501 Train Loss: 0.2338 Train R²: 0.7710 Test Loss: 0.3075 Test R²: 0.6626\n",
      "502 Train Loss: 0.2013 Train R²: 0.8026 Test Loss: 0.3035 Test R²: 0.6671\n",
      "503 Train Loss: 0.1742 Train R²: 0.8283 Test Loss: 0.2896 Test R²: 0.6823\n",
      "504 Train Loss: 0.2155 Train R²: 0.7914 Test Loss: 0.2994 Test R²: 0.6715\n",
      "505 Train Loss: 0.2307 Train R²: 0.7753 Test Loss: 0.2944 Test R²: 0.6770\n",
      "506 Train Loss: 0.1919 Train R²: 0.8120 Test Loss: 0.3592 Test R²: 0.6060\n",
      "507 Train Loss: 0.1712 Train R²: 0.8310 Test Loss: 0.3162 Test R²: 0.6532\n",
      "508 Train Loss: 0.2186 Train R²: 0.7857 Test Loss: 0.2790 Test R²: 0.6939\n",
      "509 Train Loss: 0.1968 Train R²: 0.8068 Test Loss: 0.2854 Test R²: 0.6869\n",
      "510 Train Loss: 0.1842 Train R²: 0.8196 Test Loss: 0.2970 Test R²: 0.6742\n",
      "511 Train Loss: 0.2412 Train R²: 0.7649 Test Loss: 0.3041 Test R²: 0.6664\n",
      "512 Train Loss: 0.1805 Train R²: 0.8229 Test Loss: 0.3053 Test R²: 0.6650\n",
      "513 Train Loss: 0.2222 Train R²: 0.7823 Test Loss: 0.3133 Test R²: 0.6563\n",
      "514 Train Loss: 0.1820 Train R²: 0.8227 Test Loss: 0.3402 Test R²: 0.6268\n",
      "515 Train Loss: 0.1883 Train R²: 0.8173 Test Loss: 0.3138 Test R²: 0.6557\n",
      "516 Train Loss: 0.2055 Train R²: 0.7985 Test Loss: 0.2912 Test R²: 0.6806\n",
      "517 Train Loss: 0.2314 Train R²: 0.7743 Test Loss: 0.3097 Test R²: 0.6602\n",
      "518 Train Loss: 0.1960 Train R²: 0.8080 Test Loss: 0.3327 Test R²: 0.6350\n",
      "519 Train Loss: 0.2076 Train R²: 0.7985 Test Loss: 0.2835 Test R²: 0.6890\n",
      "520 Train Loss: 0.2227 Train R²: 0.7812 Test Loss: 0.3279 Test R²: 0.6403\n",
      "521 Train Loss: 0.1910 Train R²: 0.8122 Test Loss: 0.3213 Test R²: 0.6475\n",
      "522 Train Loss: 0.2024 Train R²: 0.8017 Test Loss: 0.3201 Test R²: 0.6488\n",
      "523 Train Loss: 0.1950 Train R²: 0.8076 Test Loss: 0.3099 Test R²: 0.6600\n",
      "524 Train Loss: 0.2026 Train R²: 0.8018 Test Loss: 0.3134 Test R²: 0.6562\n",
      "525 Train Loss: 0.2207 Train R²: 0.7843 Test Loss: 0.3290 Test R²: 0.6390\n",
      "526 Train Loss: 0.1905 Train R²: 0.8134 Test Loss: 0.3208 Test R²: 0.6481\n",
      "527 Train Loss: 0.2013 Train R²: 0.8055 Test Loss: 0.3014 Test R²: 0.6694\n",
      "528 Train Loss: 0.1809 Train R²: 0.8232 Test Loss: 0.3219 Test R²: 0.6468\n",
      "529 Train Loss: 0.1613 Train R²: 0.8422 Test Loss: 0.3119 Test R²: 0.6579\n",
      "530 Train Loss: 0.1889 Train R²: 0.8145 Test Loss: 0.3232 Test R²: 0.6455\n",
      "531 Train Loss: 0.1805 Train R²: 0.8231 Test Loss: 0.2937 Test R²: 0.6778\n",
      "532 Train Loss: 0.1557 Train R²: 0.8466 Test Loss: 0.3125 Test R²: 0.6571\n",
      "533 Train Loss: 0.2098 Train R²: 0.7930 Test Loss: 0.3205 Test R²: 0.6484\n",
      "534 Train Loss: 0.2159 Train R²: 0.7875 Test Loss: 0.2951 Test R²: 0.6762\n",
      "535 Train Loss: 0.1704 Train R²: 0.8330 Test Loss: 0.2969 Test R²: 0.6743\n",
      "536 Train Loss: 0.1770 Train R²: 0.8272 Test Loss: 0.3205 Test R²: 0.6484\n",
      "537 Train Loss: 0.1884 Train R²: 0.8151 Test Loss: 0.3239 Test R²: 0.6447\n",
      "538 Train Loss: 0.1992 Train R²: 0.8043 Test Loss: 0.2931 Test R²: 0.6785\n",
      "539 Train Loss: 0.1902 Train R²: 0.8144 Test Loss: 0.3318 Test R²: 0.6360\n",
      "540 Train Loss: 0.1776 Train R²: 0.8256 Test Loss: 0.2995 Test R²: 0.6715\n",
      "541 Train Loss: 0.1756 Train R²: 0.8287 Test Loss: 0.3077 Test R²: 0.6625\n",
      "542 Train Loss: 0.1653 Train R²: 0.8377 Test Loss: 0.3054 Test R²: 0.6650\n",
      "543 Train Loss: 0.2341 Train R²: 0.7772 Test Loss: 0.3319 Test R²: 0.6359\n",
      "544 Train Loss: 0.1386 Train R²: 0.8638 Test Loss: 0.3199 Test R²: 0.6490\n",
      "545 Train Loss: 0.1679 Train R²: 0.8341 Test Loss: 0.3132 Test R²: 0.6564\n",
      "546 Train Loss: 0.1874 Train R²: 0.8175 Test Loss: 0.3101 Test R²: 0.6598\n",
      "547 Train Loss: 0.1547 Train R²: 0.8481 Test Loss: 0.3369 Test R²: 0.6304\n",
      "548 Train Loss: 0.1728 Train R²: 0.8306 Test Loss: 0.3884 Test R²: 0.5739\n",
      "549 Train Loss: 0.1565 Train R²: 0.8462 Test Loss: 0.2969 Test R²: 0.6743\n",
      "550 Train Loss: 0.1573 Train R²: 0.8476 Test Loss: 0.3048 Test R²: 0.6657\n",
      "551 Train Loss: 0.1352 Train R²: 0.8671 Test Loss: 0.3010 Test R²: 0.6698\n",
      "552 Train Loss: 0.1796 Train R²: 0.8240 Test Loss: 0.3015 Test R²: 0.6693\n",
      "553 Train Loss: 0.1481 Train R²: 0.8554 Test Loss: 0.2989 Test R²: 0.6721\n",
      "554 Train Loss: 0.1791 Train R²: 0.8228 Test Loss: 0.3260 Test R²: 0.6423\n",
      "555 Train Loss: 0.1485 Train R²: 0.8554 Test Loss: 0.3259 Test R²: 0.6425\n",
      "556 Train Loss: 0.1874 Train R²: 0.8184 Test Loss: 0.4189 Test R²: 0.5405\n",
      "557 Train Loss: 0.1613 Train R²: 0.8419 Test Loss: 0.2802 Test R²: 0.6926\n",
      "558 Train Loss: 0.2050 Train R²: 0.8005 Test Loss: 0.2906 Test R²: 0.6812\n",
      "559 Train Loss: 0.1958 Train R²: 0.8097 Test Loss: 0.3056 Test R²: 0.6648\n",
      "560 Train Loss: 0.1615 Train R²: 0.8427 Test Loss: 0.2991 Test R²: 0.6718\n",
      "561 Train Loss: 0.1636 Train R²: 0.8400 Test Loss: 0.3378 Test R²: 0.6294\n",
      "562 Train Loss: 0.1543 Train R²: 0.8481 Test Loss: 0.3416 Test R²: 0.6253\n",
      "563 Train Loss: 0.2088 Train R²: 0.7955 Test Loss: 0.2942 Test R²: 0.6772\n",
      "564 Train Loss: 0.1830 Train R²: 0.8215 Test Loss: 0.3106 Test R²: 0.6593\n",
      "565 Train Loss: 0.1875 Train R²: 0.8156 Test Loss: 0.3012 Test R²: 0.6696\n",
      "566 Train Loss: 0.1457 Train R²: 0.8593 Test Loss: 0.3158 Test R²: 0.6536\n",
      "567 Train Loss: 0.1673 Train R²: 0.8370 Test Loss: 0.3506 Test R²: 0.6154\n",
      "568 Train Loss: 0.1754 Train R²: 0.8285 Test Loss: 0.3100 Test R²: 0.6599\n",
      "569 Train Loss: 0.1616 Train R²: 0.8421 Test Loss: 0.3106 Test R²: 0.6593\n",
      "570 Train Loss: 0.1558 Train R²: 0.8482 Test Loss: 0.3222 Test R²: 0.6465\n",
      "571 Train Loss: 0.1653 Train R²: 0.8401 Test Loss: 0.3642 Test R²: 0.6004\n",
      "572 Train Loss: 0.1484 Train R²: 0.8554 Test Loss: 0.3046 Test R²: 0.6659\n",
      "573 Train Loss: 0.1659 Train R²: 0.8376 Test Loss: 0.3401 Test R²: 0.6269\n",
      "574 Train Loss: 0.1874 Train R²: 0.8167 Test Loss: 0.3625 Test R²: 0.6024\n",
      "575 Train Loss: 0.1593 Train R²: 0.8439 Test Loss: 0.3489 Test R²: 0.6173\n",
      "576 Train Loss: 0.1522 Train R²: 0.8507 Test Loss: 0.3169 Test R²: 0.6523\n",
      "577 Train Loss: 0.1995 Train R²: 0.8035 Test Loss: 0.3320 Test R²: 0.6358\n",
      "578 Train Loss: 0.1453 Train R²: 0.8570 Test Loss: 0.3409 Test R²: 0.6260\n",
      "579 Train Loss: 0.1603 Train R²: 0.8435 Test Loss: 0.3397 Test R²: 0.6273\n",
      "580 Train Loss: 0.1573 Train R²: 0.8462 Test Loss: 0.3314 Test R²: 0.6365\n",
      "581 Train Loss: 0.1608 Train R²: 0.8425 Test Loss: 0.3445 Test R²: 0.6220\n",
      "582 Train Loss: 0.1501 Train R²: 0.8534 Test Loss: 0.3147 Test R²: 0.6548\n",
      "583 Train Loss: 0.1666 Train R²: 0.8357 Test Loss: 0.2883 Test R²: 0.6838\n",
      "584 Train Loss: 0.1687 Train R²: 0.8368 Test Loss: 0.3296 Test R²: 0.6384\n",
      "585 Train Loss: 0.1883 Train R²: 0.8154 Test Loss: 0.3287 Test R²: 0.6394\n",
      "586 Train Loss: 0.1660 Train R²: 0.8368 Test Loss: 0.3185 Test R²: 0.6506\n",
      "587 Train Loss: 0.1646 Train R²: 0.8411 Test Loss: 0.3205 Test R²: 0.6484\n",
      "588 Train Loss: 0.1400 Train R²: 0.8626 Test Loss: 0.3007 Test R²: 0.6701\n",
      "589 Train Loss: 0.1920 Train R²: 0.8128 Test Loss: 0.3378 Test R²: 0.6294\n",
      "590 Train Loss: 0.1794 Train R²: 0.8246 Test Loss: 0.3144 Test R²: 0.6551\n",
      "591 Train Loss: 0.1775 Train R²: 0.8251 Test Loss: 0.3553 Test R²: 0.6102\n",
      "592 Train Loss: 0.1882 Train R²: 0.8146 Test Loss: 0.3725 Test R²: 0.5913\n",
      "593 Train Loss: 0.2358 Train R²: 0.7704 Test Loss: 0.3425 Test R²: 0.6242\n",
      "594 Train Loss: 0.1632 Train R²: 0.8397 Test Loss: 0.3388 Test R²: 0.6284\n",
      "595 Train Loss: 0.2029 Train R²: 0.8005 Test Loss: 0.3445 Test R²: 0.6220\n",
      "596 Train Loss: 0.1790 Train R²: 0.8244 Test Loss: 0.3452 Test R²: 0.6213\n",
      "597 Train Loss: 0.1953 Train R²: 0.8091 Test Loss: 0.3459 Test R²: 0.6205\n",
      "598 Train Loss: 0.1676 Train R²: 0.8350 Test Loss: 0.3111 Test R²: 0.6587\n",
      "599 Train Loss: 0.1654 Train R²: 0.8372 Test Loss: 0.3231 Test R²: 0.6455\n"
     ]
    }
   ],
   "source": [
    "# simple training and evaluation loop to train Attentive FP model \n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "EPOCHS = 600\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    loss_list_train = []\n",
    "    pred_list_train = []  \n",
    "    true_list_train = []  \n",
    "    \n",
    "    model.train()\n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        output = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "\n",
    "        loss = loss_function(output, data.y)\n",
    "        loss.backward()  \n",
    "        loss_list_train.append(loss.item())\n",
    "        optimizer.step()  \n",
    "        \n",
    "        # Store predictions \n",
    "        pred_list_train.extend(output.detach().cpu().numpy().flatten())\n",
    "        true_list_train.extend(data.y.cpu().numpy().flatten())\n",
    "    \n",
    "    # Calculate R² \n",
    "    r2_train = r2_score(true_list_train, pred_list_train)\n",
    "        \n",
    "    loss_list_test = []\n",
    "    pred_list_test = []  \n",
    "    true_list_test = []  \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            output = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "\n",
    "            loss = loss_function(output, data.y)\n",
    "            loss_list_test.append(loss.item())\n",
    "     \n",
    "            # Store predictions \n",
    "            pred_list_test.extend(output.cpu().numpy().flatten())\n",
    "            true_list_test.extend(data.y.cpu().numpy().flatten())\n",
    "    \n",
    "    # Calculate R²  \n",
    "    r2_test = r2_score(true_list_test, pred_list_test)\n",
    "                \n",
    "    print(i, \"Train Loss: %.4f Train R²: %.4f Test Loss: %.4f Test R²: %.4f\"\n",
    "        % (np.mean(loss_list_train), r2_train, \n",
    "           np.mean(loss_list_test), r2_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66c6b691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "FOLD 1/5\n",
      "==================================================\n",
      "\n",
      "Epoch 0: Train Loss: 16.2752 Train R²: -14.7845 Val Loss: 1.3063 Val R²: -0.6209\n",
      "Epoch 20: Train Loss: 0.5605 Train R²: 0.4689 Val Loss: 0.5173 Val R²: 0.3580\n",
      "Epoch 40: Train Loss: 0.4841 Train R²: 0.5387 Val Loss: 0.4878 Val R²: 0.3946\n",
      "Epoch 60: Train Loss: 0.4662 Train R²: 0.5542 Val Loss: 0.5530 Val R²: 0.3138\n",
      "Epoch 80: Train Loss: 0.3412 Train R²: 0.6729 Val Loss: 0.3029 Val R²: 0.6242\n",
      "Epoch 100: Train Loss: 0.2735 Train R²: 0.7378 Val Loss: 0.2745 Val R²: 0.6593\n",
      "Epoch 120: Train Loss: 0.2240 Train R²: 0.7851 Val Loss: 0.2388 Val R²: 0.7037\n",
      "Epoch 140: Train Loss: 0.2294 Train R²: 0.7792 Val Loss: 0.1934 Val R²: 0.7600\n",
      "Epoch 160: Train Loss: 0.1613 Train R²: 0.8446 Val Loss: 0.1960 Val R²: 0.7568\n",
      "Epoch 180: Train Loss: 0.1218 Train R²: 0.8835 Val Loss: 0.1991 Val R²: 0.7529\n",
      "Early stopping at epoch 197\n",
      "\n",
      "Fold 1 Results:\n",
      "Best Val Loss: 0.1679\n",
      "Best Val R²: 0.7917\n",
      "\n",
      "==================================================\n",
      "FOLD 2/5\n",
      "==================================================\n",
      "\n",
      "Epoch 0: Train Loss: 82.1585 Train R²: -83.7337 Val Loss: 5.7471 Val R²: -4.3391\n",
      "Epoch 20: Train Loss: 0.5630 Train R²: 0.4277 Val Loss: 0.5056 Val R²: 0.5282\n",
      "Epoch 40: Train Loss: 0.4366 Train R²: 0.5541 Val Loss: 0.3480 Val R²: 0.6750\n",
      "Epoch 60: Train Loss: 0.4501 Train R²: 0.5394 Val Loss: 0.6406 Val R²: 0.4031\n",
      "Epoch 80: Train Loss: 0.3204 Train R²: 0.6717 Val Loss: 0.3853 Val R²: 0.6410\n",
      "Epoch 100: Train Loss: 0.2511 Train R²: 0.7427 Val Loss: 0.3678 Val R²: 0.6580\n",
      "Epoch 120: Train Loss: 0.2172 Train R²: 0.7785 Val Loss: 0.3607 Val R²: 0.6642\n",
      "Epoch 140: Train Loss: 0.1671 Train R²: 0.8294 Val Loss: 0.3433 Val R²: 0.6802\n",
      "Epoch 160: Train Loss: 0.1302 Train R²: 0.8674 Val Loss: 0.2125 Val R²: 0.8020\n",
      "Epoch 180: Train Loss: 0.1052 Train R²: 0.8935 Val Loss: 0.2615 Val R²: 0.7570\n",
      "Epoch 200: Train Loss: 0.1080 Train R²: 0.8901 Val Loss: 0.2132 Val R²: 0.8019\n",
      "Epoch 220: Train Loss: 0.0871 Train R²: 0.9113 Val Loss: 0.2546 Val R²: 0.7636\n",
      "Early stopping at epoch 236\n",
      "\n",
      "Fold 2 Results:\n",
      "Best Val Loss: 0.1707\n",
      "Best Val R²: 0.8411\n",
      "\n",
      "==================================================\n",
      "FOLD 3/5\n",
      "==================================================\n",
      "\n",
      "Epoch 0: Train Loss: 13.2010 Train R²: -11.3311 Val Loss: 0.5692 Val R²: 0.1429\n",
      "Epoch 20: Train Loss: 0.4492 Train R²: 0.5846 Val Loss: 0.6380 Val R²: 0.0426\n",
      "Epoch 40: Train Loss: 0.4343 Train R²: 0.5981 Val Loss: 0.4645 Val R²: 0.3016\n",
      "Epoch 60: Train Loss: 0.4153 Train R²: 0.6161 Val Loss: 0.4504 Val R²: 0.3224\n",
      "Early stopping at epoch 76\n",
      "\n",
      "Fold 3 Results:\n",
      "Best Val Loss: 0.3404\n",
      "Best Val R²: 0.4888\n",
      "\n",
      "==================================================\n",
      "FOLD 4/5\n",
      "==================================================\n",
      "\n",
      "Epoch 0: Train Loss: 11.1325 Train R²: -10.4835 Val Loss: 1.0880 Val R²: 0.0037\n",
      "Epoch 20: Train Loss: 0.5160 Train R²: 0.4720 Val Loss: 0.6056 Val R²: 0.4429\n",
      "Epoch 40: Train Loss: 0.3109 Train R²: 0.6849 Val Loss: 0.6185 Val R²: 0.4310\n",
      "Epoch 60: Train Loss: 0.2547 Train R²: 0.7398 Val Loss: 0.5246 Val R²: 0.5182\n",
      "Epoch 80: Train Loss: 0.2125 Train R²: 0.7831 Val Loss: 0.5265 Val R²: 0.5163\n",
      "Epoch 100: Train Loss: 0.1488 Train R²: 0.8485 Val Loss: 0.3969 Val R²: 0.6353\n",
      "Epoch 120: Train Loss: 0.1289 Train R²: 0.8687 Val Loss: 0.3929 Val R²: 0.6394\n",
      "Epoch 140: Train Loss: 0.1028 Train R²: 0.8951 Val Loss: 0.3657 Val R²: 0.6645\n",
      "Epoch 160: Train Loss: 0.0783 Train R²: 0.9205 Val Loss: 0.3829 Val R²: 0.6487\n",
      "Epoch 180: Train Loss: 0.0523 Train R²: 0.9467 Val Loss: 0.3804 Val R²: 0.6511\n",
      "Epoch 200: Train Loss: 0.0531 Train R²: 0.9462 Val Loss: 0.3014 Val R²: 0.7236\n",
      "Epoch 220: Train Loss: 0.0507 Train R²: 0.9481 Val Loss: 0.3162 Val R²: 0.7104\n",
      "Epoch 240: Train Loss: 0.0424 Train R²: 0.9566 Val Loss: 0.2929 Val R²: 0.7315\n",
      "Epoch 260: Train Loss: 0.0353 Train R²: 0.9639 Val Loss: 0.3086 Val R²: 0.7172\n",
      "Epoch 280: Train Loss: 0.0365 Train R²: 0.9625 Val Loss: 0.2921 Val R²: 0.7323\n",
      "Early stopping at epoch 284\n",
      "\n",
      "Fold 4 Results:\n",
      "Best Val Loss: 0.2829\n",
      "Best Val R²: 0.7406\n",
      "\n",
      "==================================================\n",
      "FOLD 5/5\n",
      "==================================================\n",
      "\n",
      "Epoch 0: Train Loss: 32.7158 Train R²: -35.3193 Val Loss: 4.7843 Val R²: -2.5957\n",
      "Epoch 20: Train Loss: 0.6017 Train R²: 0.3388 Val Loss: 0.8223 Val R²: 0.3773\n",
      "Epoch 40: Train Loss: 0.4756 Train R²: 0.4774 Val Loss: 0.6827 Val R²: 0.4842\n",
      "Epoch 60: Train Loss: 0.4655 Train R²: 0.4889 Val Loss: 0.6416 Val R²: 0.5150\n",
      "Epoch 80: Train Loss: 0.3669 Train R²: 0.5995 Val Loss: 0.5233 Val R²: 0.6036\n",
      "Epoch 100: Train Loss: 0.3183 Train R²: 0.6520 Val Loss: 0.5692 Val R²: 0.5686\n",
      "Epoch 120: Train Loss: 0.2365 Train R²: 0.7397 Val Loss: 0.4252 Val R²: 0.6776\n",
      "Epoch 140: Train Loss: 0.1949 Train R²: 0.7867 Val Loss: 0.4572 Val R²: 0.6534\n",
      "Epoch 160: Train Loss: 0.1465 Train R²: 0.8393 Val Loss: 0.3912 Val R²: 0.7036\n",
      "Epoch 180: Train Loss: 0.1503 Train R²: 0.8351 Val Loss: 0.3621 Val R²: 0.7256\n",
      "Epoch 200: Train Loss: 0.0975 Train R²: 0.8931 Val Loss: 0.3994 Val R²: 0.6973\n",
      "Epoch 220: Train Loss: 0.0815 Train R²: 0.9109 Val Loss: 0.4038 Val R²: 0.6941\n",
      "Early stopping at epoch 230\n",
      "\n",
      "Fold 5 Results:\n",
      "Best Val Loss: 0.3621\n",
      "Best Val R²: 0.7256\n",
      "\n",
      "==================================================\n",
      "CROSS-VALIDATION SUMMARY\n",
      "==================================================\n",
      "Mean Val Loss: 0.2648 ± 0.0822\n",
      "Mean Val R²: 0.7176 ± 0.1214\n",
      "\n",
      "Individual Folds:\n",
      "  Fold 1: Loss=0.1679, R²=0.7917\n",
      "  Fold 2: Loss=0.1707, R²=0.8411\n",
      "  Fold 3: Loss=0.3404, R²=0.4888\n",
      "  Fold 4: Loss=0.2829, R²=0.7406\n",
      "  Fold 5: Loss=0.3621, R²=0.7256\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "fold_models = []\n",
    "\n",
    "# 5-fold cross validation\n",
    "\n",
    "# Setup\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_results = []\n",
    "EPOCHS = 600\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(graph_list)):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"FOLD {fold + 1}/5\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    # Create fresh model for this fold\n",
    "    model = AttentiveFP(in_channels=16,hidden_channels=200,out_channels=1,edge_dim=3,num_layers=4,num_timesteps=3,dropout=0.095)\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=0.0013,weight_decay=0.00014)\n",
    "    loss_function = torch.nn.MSELoss()  # Or your loss function\n",
    "    \n",
    "#hidden_channels': 200, 'num_layers': 4, 'num_timesteps': 3, 'dropout': 0.09517795284321594, 'lr': 0.0013269356447544063, 'weight_decay': 0.00014218285610078816, 'batch_size': 16\n",
    "\n",
    "    # Split data for this fold\n",
    "    train_graphs = [graph_list[i] for i in train_idx]\n",
    "    val_graphs = [graph_list[i] for i in val_idx]\n",
    "    \n",
    "    train_loader = DataLoader(train_graphs, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_graphs, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # Track best model for this fold\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "    patience = 50\n",
    "    \n",
    "    # Training loop for this fold\n",
    "    for epoch in range(EPOCHS):\n",
    "        # TRAINING\n",
    "        loss_list_train = []\n",
    "        pred_list_train = []  \n",
    "        true_list_train = []  \n",
    "        \n",
    "        model.train()\n",
    "        for data in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            output = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "            loss = loss_function(output, data.y)\n",
    "            loss.backward()  \n",
    "            loss_list_train.append(loss.item())\n",
    "            optimizer.step()  \n",
    "            \n",
    "            pred_list_train.extend(output.detach().cpu().numpy().flatten())\n",
    "            true_list_train.extend(data.y.cpu().numpy().flatten())\n",
    "        \n",
    "        r2_train = r2_score(true_list_train, pred_list_train)\n",
    "        mean_train_loss = np.mean(loss_list_train)\n",
    "            \n",
    "        # VALIDATION\n",
    "        loss_list_val = []\n",
    "        pred_list_val = []  \n",
    "        true_list_val = []  \n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                output = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "                loss = loss_function(output, data.y)\n",
    "                loss_list_val.append(loss.item())\n",
    "         \n",
    "                pred_list_val.extend(output.cpu().numpy().flatten())\n",
    "                true_list_val.extend(data.y.cpu().numpy().flatten())\n",
    "        \n",
    "        r2_val = r2_score(true_list_val, pred_list_val)\n",
    "        mean_val_loss = np.mean(loss_list_val)\n",
    "        \n",
    "        # Save best model\n",
    "        if mean_val_loss < best_val_loss:\n",
    "            best_val_loss = mean_val_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            best_r2_val = r2_val\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "                    \n",
    "        if epoch % 20 == 0:  # Print every 20 epochs\n",
    "            print(f\"Epoch {epoch}: Train Loss: {mean_train_loss:.4f} Train R²: {r2_train:.4f} \"\n",
    "                  f\"Val Loss: {mean_val_loss:.4f} Val R²: {r2_val:.4f}\")\n",
    "            \n",
    "\n",
    "    # Load best model state for this fold\n",
    "    model.load_state_dict(best_model_state)\n",
    "    \n",
    "    # Save the model\n",
    "    torch.save(best_model_state, f'attentivefp_fold_{fold+1}.pt')\n",
    "    fold_models.append(copy.deepcopy(model))  # Keep in memory too\n",
    "    \n",
    "    # Store results for this fold\n",
    "    fold_results.append({\n",
    "        'fold': fold + 1,\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'best_val_r2': best_r2_val,\n",
    "        'model': model  # Optional: keep reference\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nFold {fold + 1} Results:\")\n",
    "    print(f\"Best Val Loss: {best_val_loss:.4f}\")\n",
    "    print(f\"Best Val R²: {best_r2_val:.4f}\")\n",
    "\n",
    "# Summary across all folds\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"CROSS-VALIDATION SUMMARY\")\n",
    "print(f\"{'='*50}\")\n",
    "val_losses = [f['best_val_loss'] for f in fold_results]\n",
    "val_r2s = [f['best_val_r2'] for f in fold_results]\n",
    "\n",
    "print(f\"Mean Val Loss: {np.mean(val_losses):.4f} ± {np.std(val_losses):.4f}\")\n",
    "print(f\"Mean Val R²: {np.mean(val_r2s):.4f} ± {np.std(val_r2s):.4f}\")\n",
    "print(f\"\\nIndividual Folds:\")\n",
    "for result in fold_results:\n",
    "    print(f\"  Fold {result['fold']}: Loss={result['best_val_loss']:.4f}, R²={result['best_val_r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9237a3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "[{'fold': 1, 'best_val_loss': np.float64(0.24105320125818253), 'best_val_r2': 0.7008807884380686}, {'fold': 2, 'best_val_loss': np.float64(0.2490873783826828), 'best_val_r2': 0.7682296322880371}, {'fold': 3, 'best_val_loss': np.float64(0.20825927704572678), 'best_val_r2': 0.6877712445998967}, {'fold': 4, 'best_val_loss': np.float64(0.2778569757938385), 'best_val_r2': 0.744636401399734}, {'fold': 5, 'best_val_loss': np.float64(0.4175241142511368), 'best_val_r2': 0.6831592452897886}]\n"
     ]
    }
   ],
   "source": [
    "# See if best_model_state exists\n",
    "print('best_model_state' in dir())\n",
    "\n",
    "# Or check fold_results\n",
    "print(fold_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7de92532",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-29 13:38:53,059] A new study created in memory with name: no-name-26591616-0f4a-4834-ae7c-c0eab92185d1\n",
      "[I 2026-01-29 13:41:27,356] Trial 0 finished with value: 0.2766966037452221 and parameters: {'hidden_channels': 128, 'num_layers': 4, 'num_timesteps': 3, 'dropout': 0.23882434565022131, 'lr': 0.0015923126382256317, 'weight_decay': 7.454973991521727e-05, 'batch_size': 16}. Best is trial 0 with value: 0.2766966037452221.\n",
      "[I 2026-01-29 13:43:38,188] Trial 1 finished with value: 0.31975647807121277 and parameters: {'hidden_channels': 128, 'num_layers': 2, 'num_timesteps': 3, 'dropout': 0.43611140282317074, 'lr': 0.0005566502852715476, 'weight_decay': 0.0002564288066972483, 'batch_size': 16}. Best is trial 0 with value: 0.2766966037452221.\n",
      "[I 2026-01-29 13:44:51,441] Trial 2 finished with value: 0.22972967475652695 and parameters: {'hidden_channels': 32, 'num_layers': 4, 'num_timesteps': 3, 'dropout': 0.12261492587333833, 'lr': 0.0019778710959787536, 'weight_decay': 1.3516398871495241e-05, 'batch_size': 16}. Best is trial 2 with value: 0.22972967475652695.\n",
      "[I 2026-01-29 13:45:40,522] Trial 3 finished with value: 0.4315924346446991 and parameters: {'hidden_channels': 64, 'num_layers': 4, 'num_timesteps': 2, 'dropout': 0.38698988309002463, 'lr': 0.0007225520722094631, 'weight_decay': 1.3336777684163681e-05, 'batch_size': 32}. Best is trial 2 with value: 0.22972967475652695.\n",
      "[I 2026-01-29 13:48:15,834] Trial 4 finished with value: 0.550551176071167 and parameters: {'hidden_channels': 200, 'num_layers': 5, 'num_timesteps': 2, 'dropout': 0.3315052124914086, 'lr': 0.005248053777460544, 'weight_decay': 7.317714135897972e-05, 'batch_size': 64}. Best is trial 2 with value: 0.22972967475652695.\n",
      "[I 2026-01-29 13:48:16,776] Trial 5 pruned. \n",
      "[I 2026-01-29 13:48:17,378] Trial 6 pruned. \n",
      "[I 2026-01-29 13:48:29,722] Trial 7 finished with value: 0.46390967071056366 and parameters: {'hidden_channels': 64, 'num_layers': 3, 'num_timesteps': 3, 'dropout': 0.1815865304933864, 'lr': 0.002592991097953542, 'weight_decay': 8.260220138643542e-05, 'batch_size': 32}. Best is trial 2 with value: 0.22972967475652695.\n",
      "[I 2026-01-29 13:48:30,087] Trial 8 pruned. \n",
      "[I 2026-01-29 13:48:31,002] Trial 9 pruned. \n",
      "[I 2026-01-29 13:48:32,130] Trial 10 pruned. \n",
      "[I 2026-01-29 13:48:34,851] Trial 11 pruned. \n",
      "[I 2026-01-29 13:48:36,853] Trial 12 pruned. \n",
      "[I 2026-01-29 13:48:39,155] Trial 13 pruned. \n",
      "[I 2026-01-29 13:48:44,074] Trial 14 pruned. \n",
      "[I 2026-01-29 13:48:45,861] Trial 15 pruned. \n",
      "[I 2026-01-29 13:48:46,150] Trial 16 pruned. \n",
      "[I 2026-01-29 13:50:41,088] Trial 17 finished with value: 0.2723352052271366 and parameters: {'hidden_channels': 128, 'num_layers': 5, 'num_timesteps': 2, 'dropout': 0.1023595191485829, 'lr': 0.004835771482040342, 'weight_decay': 1.1296288262479282e-05, 'batch_size': 16}. Best is trial 2 with value: 0.22972967475652695.\n",
      "[I 2026-01-29 13:50:48,073] Trial 18 pruned. \n",
      "[I 2026-01-29 13:50:49,329] Trial 19 pruned. \n",
      "[I 2026-01-29 13:50:50,686] Trial 20 pruned. \n",
      "[I 2026-01-29 13:50:56,053] Trial 21 pruned. \n",
      "[I 2026-01-29 13:50:57,492] Trial 22 pruned. \n",
      "[I 2026-01-29 13:50:58,938] Trial 23 pruned. \n",
      "[I 2026-01-29 13:51:05,142] Trial 24 pruned. \n",
      "[I 2026-01-29 13:51:07,456] Trial 25 pruned. \n",
      "[I 2026-01-29 13:51:10,015] Trial 26 pruned. \n",
      "[I 2026-01-29 13:51:20,678] Trial 27 pruned. \n",
      "[I 2026-01-29 13:51:24,148] Trial 28 pruned. \n",
      "[I 2026-01-29 13:51:25,994] Trial 29 pruned. \n",
      "[I 2026-01-29 13:51:26,711] Trial 30 pruned. \n",
      "[I 2026-01-29 13:51:30,431] Trial 31 pruned. \n",
      "[I 2026-01-29 13:51:32,238] Trial 32 pruned. \n",
      "[I 2026-01-29 13:51:33,243] Trial 33 pruned. \n",
      "[I 2026-01-29 13:51:35,219] Trial 34 pruned. \n",
      "[I 2026-01-29 13:51:36,409] Trial 35 pruned. \n",
      "[I 2026-01-29 13:51:36,879] Trial 36 pruned. \n",
      "[I 2026-01-29 13:56:56,264] Trial 37 finished with value: 0.20585646480321884 and parameters: {'hidden_channels': 200, 'num_layers': 4, 'num_timesteps': 3, 'dropout': 0.09517795284321594, 'lr': 0.0013269356447544063, 'weight_decay': 0.00014218285610078816, 'batch_size': 16}. Best is trial 37 with value: 0.20585646480321884.\n",
      "[I 2026-01-29 13:56:58,656] Trial 38 pruned. \n",
      "[I 2026-01-29 13:57:00,793] Trial 39 pruned. \n",
      "[I 2026-01-29 13:57:09,707] Trial 40 pruned. \n",
      "[I 2026-01-29 13:57:17,019] Trial 41 pruned. \n",
      "[I 2026-01-29 13:57:17,730] Trial 42 pruned. \n",
      "[I 2026-01-29 13:57:19,098] Trial 43 pruned. \n",
      "[I 2026-01-29 13:57:19,412] Trial 44 pruned. \n",
      "[I 2026-01-29 13:57:26,069] Trial 45 pruned. \n",
      "[I 2026-01-29 13:57:27,254] Trial 46 pruned. \n",
      "[I 2026-01-29 13:57:29,748] Trial 47 pruned. \n",
      "[I 2026-01-29 13:57:31,073] Trial 48 pruned. \n",
      "[I 2026-01-29 13:57:36,345] Trial 49 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best AttentiveFP hyperparameters:\n",
      "{'hidden_channels': 200, 'num_layers': 4, 'num_timesteps': 3, 'dropout': 0.09517795284321594, 'lr': 0.0013269356447544063, 'weight_decay': 0.00014218285610078816, 'batch_size': 16}\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning with train-test split\n",
    "import optuna\n",
    "from optuna.trial import Trial\n",
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Create train-test split once (80-20 split)\n",
    "train_idx, test_idx = train_test_split(\n",
    "    range(len(graph_list)), \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_graphs = [graph_list[i] for i in train_idx]\n",
    "test_graphs = [graph_list[i] for i in test_idx]\n",
    "\n",
    "def objective_attentivefp(trial: Trial):\n",
    "    \"\"\"Optuna objective for AttentiveFP\"\"\"\n",
    "    \n",
    "    # AttentiveFP-specific hyperparameters\n",
    "    hidden_channels = trial.suggest_categorical('hidden_channels', [32, 64, 128, 200])\n",
    "    num_layers = trial.suggest_int('num_layers', 2, 5)\n",
    "    num_timesteps = trial.suggest_int('num_timesteps', 1, 3)\n",
    "    dropout = trial.suggest_float('dropout', 0.0, 0.5)\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-5, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
    "    \n",
    "    # Create AttentiveFP model\n",
    "    model = AttentiveFP(\n",
    "        in_channels=16,\n",
    "        hidden_channels=hidden_channels,\n",
    "        out_channels=1,\n",
    "        edge_dim=3,\n",
    "        num_layers=num_layers,\n",
    "        num_timesteps=num_timesteps,\n",
    "        dropout=dropout\n",
    "    )\n",
    "    \n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    \n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(train_graphs, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_graphs, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Training\n",
    "    best_test_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(150):\n",
    "        # Train\n",
    "        model.train()\n",
    "        for data in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "            loss = torch.nn.functional.mse_loss(out, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Test\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data in test_loader:\n",
    "                out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "                loss = torch.nn.functional.mse_loss(out, data.y)\n",
    "                test_loss += loss.item()\n",
    "        test_loss /= len(test_loader)\n",
    "        \n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= 20:\n",
    "            break\n",
    "        \n",
    "        # Pruning\n",
    "        trial.report(test_loss, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "    \n",
    "    return best_test_loss\n",
    "\n",
    "# Run optimization\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective_attentivefp, n_trials=50)\n",
    "\n",
    "print(\"\\nBest AttentiveFP hyperparameters:\")\n",
    "print(study.best_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
